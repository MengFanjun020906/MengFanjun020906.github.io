<!DOCTYPE html>
<html lang="en">
    <head hexo-theme='https://github.com/volantis-x/hexo-theme-volantis/#5.7.7'>
  <meta name="generator" content="Hexo 5.4.2">
  <meta name="Volantis" content="5.7.7">
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
  <link rel="canonical" href="https://mengfanjun020906.github.io/2025/08/30/standford cs336（二）训练模型介绍/"/>
  <!-- 渲染优化 -->
    <meta http-equiv='x-dns-prefetch-control' content='on' />
      <link rel='dns-prefetch' href='https://unpkg.com'>
      <link rel="preconnect" href="https://unpkg.com" crossorigin>
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Content-Security-Policy" content=" default-src 'self' https:; block-all-mixed-content; base-uri 'self' https:; form-action 'self' https:; worker-src 'self' https:; connect-src 'self' https: *; img-src 'self' data: https: *; media-src 'self' https: *; font-src 'self' data: https: *; frame-src 'self' https: *; manifest-src 'self' https: *; child-src https:; script-src 'self' https: 'unsafe-inline' *; style-src 'self' https: 'unsafe-inline' *; ">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
  <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style">
  <meta content="telephone=no" name="format-detection">
  <!-- import head_begin begin -->
  <!-- import head_begin end -->
  <!-- Custom Files headBegin begin-->
  
  <!-- Custom Files headBegin end-->
    <link rel="shortcut icon" type='image/x-icon' href="https://pic.imgdb.cn/item/640ad0cdf144a01007830622.png">
  <link rel="preload" href="/css/style.css" as="style">
  <link rel="preload" href="https://unpkg.com/volantis-static@0.0.1654736714924/media/fonts/VarelaRound/VarelaRound-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="https://unpkg.com/volantis-static@0.0.1654736714924/media/fonts/UbuntuMono/UbuntuMono-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <!-- feed -->
  <!-- 页面元数据 -->
  <title>Standford CS336（二）训练模型介绍 - MengFanjun的博客</title>
  <meta name="keywords" content="学习,大模型,null">
  <meta desc name="description" content="20级通信工程大学生的个人博客 - MengFanjun - MengFanjun的博客">
  
<meta property="og:type" content="article">
<meta property="og:title" content="Standford CS336（二）训练模型介绍">
<meta property="og:url" content="https://mengfanjun020906.github.io/2025/08/30/Standford%20CS336%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/index.html">
<meta property="og:site_name" content="MengFanjun的博客">
<meta property="og:description" content="github库链接：https:&#x2F;&#x2F;github.com&#x2F;stanford-cs336&#x2F;spring2025-lectures 概述本讲将讨论训练模型所需的所有基本要素，从张量到底层模型，再到优化器和训练循环。我们将密切关注效率（资源利用）。 资源类型 内存 (GB) 计算 (FLOPs)  内存核算张量基础张量是存储所有内容（参数、梯度、优化器状态、数据、激活）的基本构建块。PyTorch 提供">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png">
<meta property="article:published_time" content="2025-08-30T02:57:21.000Z">
<meta property="article:modified_time" content="2025-12-12T14:25:01.045Z">
<meta property="article:author" content="MengFanjun">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png">
  <style>
    /* 首屏样式 */
    #safearea {
  display: none;
}
:root {
  --color-site-body: #f4f4f4;
  --color-site-bg: #f4f4f4;
  --color-site-inner: #fff;
  --color-site-footer: #fff;
  --color-card: #fff;
  --color-text: #444;
  --color-block: #f6f6f6;
  --color-inlinecode: #d56d28;
  --color-codeblock: #f6f6f6;
  --color-h1: #444;
  --color-h2: #444;
  --color-h3: #444;
  --color-h4: #444;
  --color-h5: #444;
  --color-h6: #444;
  --color-p: #444;
  --color-list: #666;
  --color-list-hl: #36ac91;
  --color-meta: #888;
  --color-read-bkg: #e0d8c8;
  --color-read-post: #f8f1e2;
  --color-copyright-bkg: #f5f5f5;
}
* {
  box-sizing: border-box;
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  outline: none;
  margin: 0;
  padding: 0;
}
*::-webkit-scrollbar {
  height: 4px;
  width: 4px;
}
*::-webkit-scrollbar-track-piece {
  background: transparent;
}
*::-webkit-scrollbar-thumb {
  background: #44d7b6;
  cursor: pointer;
  border-radius: 2px;
  -webkit-border-radius: 2px;
}
*::-webkit-scrollbar-thumb:hover {
  background: #ff5722;
}
html {
  color: var(--color-text);
  width: 100%;
  height: 100%;
  font-family: UbuntuMono, "Varela Round", "PingFang SC", "Microsoft YaHei", Helvetica, Arial, Menlo, Monaco, monospace, sans-serif;
  font-size: 16px;
}
html >::-webkit-scrollbar {
  height: 4px;
  width: 4px;
}
html >::-webkit-scrollbar-track-piece {
  background: transparent;
}
html >::-webkit-scrollbar-thumb {
  background: #44d7b6;
  cursor: pointer;
  border-radius: 2px;
  -webkit-border-radius: 2px;
}
html >::-webkit-scrollbar-thumb:hover {
  background: #ff5722;
}
body {
  background-color: var(--color-site-body);
  text-rendering: optimizelegibility;
  -webkit-tap-highlight-color: rgba(0,0,0,0);
  line-height: 1.6;
  -webkit-text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
}
body.modal-active {
  overflow: hidden;
}
@media screen and (max-width: 680px) {
  body.modal-active {
    position: fixed;
    top: 0;
    right: 0;
    bottom: 0;
    left: 0;
  }
}
a {
  color: #2196f3;
  cursor: pointer;
  text-decoration: none;
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
}
a:hover {
  color: #ff5722;
}
a:active,
a:hover {
  outline: 0;
}
ul,
ol {
  padding-left: 0;
}
ul li,
ol li {
  list-style: none;
}
header {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
}
img {
  border: 0;
  background: none;
  max-width: 100%;
}
svg:not(:root) {
  overflow: hidden;
}
hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  height: 0;
  border: 0;
  border-radius: 1px;
  -webkit-border-radius: 1px;
  border-bottom: 1px solid rgba(68,68,68,0.1);
}
button,
input {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
  text-transform: none;
  -webkit-appearance: button;
  cursor: pointer;
}
@supports (backdrop-filter: blur(20px)) {
  .blur {
    background: rgba(255,255,255,0.9) !important;
    backdrop-filter: saturate(200%) blur(20px);
  }
}
.shadow {
  box-shadow: 0 1px 2px 0px rgba(0,0,0,0.1);
  -webkit-box-shadow: 0 1px 2px 0px rgba(0,0,0,0.1);
}
.shadow.floatable {
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
}
.shadow.floatable:hover {
  box-shadow: 0 2px 4px 0px rgba(0,0,0,0.1), 0 4px 8px 0px rgba(0,0,0,0.1), 0 8px 16px 0px rgba(0,0,0,0.1);
  -webkit-box-shadow: 0 2px 4px 0px rgba(0,0,0,0.1), 0 4px 8px 0px rgba(0,0,0,0.1), 0 8px 16px 0px rgba(0,0,0,0.1);
}
#l_cover {
  min-height: 64px;
}
.cover-wrapper {
  top: 0;
  left: 0;
  max-width: 100%;
  height: 100vh;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  flex-wrap: nowrap;
  -webkit-flex-wrap: nowrap;
  -khtml-flex-wrap: nowrap;
  -moz-flex-wrap: nowrap;
  -o-flex-wrap: nowrap;
  -ms-flex-wrap: nowrap;
  -webkit-box-direction: normal;
  -moz-box-direction: normal;
  -webkit-box-orient: vertical;
  -moz-box-orient: vertical;
  -webkit-flex-direction: column;
  -ms-flex-direction: column;
  flex-direction: column;
  align-items: center;
  align-self: center;
  align-content: center;
  color: var(--color-site-inner);
  padding: 0 16px;
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  position: relative;
  overflow: hidden;
  margin-bottom: -100px;
}
.cover-wrapper .cover-body {
  z-index: 1;
  position: relative;
  width: 100%;
  height: 100%;
}
.cover-wrapper#full {
  height: calc(100vh + 100px);
  padding-bottom: 100px;
}
.cover-wrapper#half {
  max-height: 640px;
  min-height: 400px;
  height: calc(36vh - 64px + 200px);
}
.cover-wrapper #scroll-down {
  width: 100%;
  height: 64px;
  position: absolute;
  bottom: 100px;
  text-align: center;
  cursor: pointer;
}
.cover-wrapper #scroll-down .scroll-down-effects {
  color: #fff;
  font-size: 24px;
  line-height: 64px;
  position: absolute;
  width: 24px;
  left: calc(50% - 12px);
  text-shadow: 0 1px 2px rgba(0,0,0,0.1);
  animation: scroll-down-effect 1.5s infinite;
  -webkit-animation: scroll-down-effect 1.5s infinite;
  -khtml-animation: scroll-down-effect 1.5s infinite;
  -moz-animation: scroll-down-effect 1.5s infinite;
  -o-animation: scroll-down-effect 1.5s infinite;
  -ms-animation: scroll-down-effect 1.5s infinite;
}
@-moz-keyframes scroll-down-effect {
  0% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
  50% {
    top: -16px;
    opacity: 0.4;
    -webkit-opacity: 0.4;
    -moz-opacity: 0.4;
  }
  100% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@-webkit-keyframes scroll-down-effect {
  0% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
  50% {
    top: -16px;
    opacity: 0.4;
    -webkit-opacity: 0.4;
    -moz-opacity: 0.4;
  }
  100% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@-o-keyframes scroll-down-effect {
  0% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
  50% {
    top: -16px;
    opacity: 0.4;
    -webkit-opacity: 0.4;
    -moz-opacity: 0.4;
  }
  100% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@keyframes scroll-down-effect {
  0% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
  50% {
    top: -16px;
    opacity: 0.4;
    -webkit-opacity: 0.4;
    -moz-opacity: 0.4;
  }
  100% {
    top: 0;
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
.cover-wrapper .cover-body {
  margin-top: 64px;
  margin-bottom: 100px;
}
.cover-wrapper .cover-body,
.cover-wrapper .cover-body .top,
.cover-wrapper .cover-body .bottom {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  -webkit-box-direction: normal;
  -moz-box-direction: normal;
  -webkit-box-orient: vertical;
  -moz-box-orient: vertical;
  -webkit-flex-direction: column;
  -ms-flex-direction: column;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  -webkit-justify-content: center;
  -khtml-justify-content: center;
  -moz-justify-content: center;
  -o-justify-content: center;
  -ms-justify-content: center;
  max-width: 100%;
}
.cover-wrapper .cover-body .bottom {
  margin-top: 32px;
}
.cover-wrapper .cover-body .title {
  font-family: "Varela Round", "PingFang SC", "Microsoft YaHei", Helvetica, Arial, Helvetica, monospace;
  font-size: 3.125rem;
  line-height: 1.2;
  text-shadow: 0 1px 2px rgba(0,0,0,0.1);
}
.cover-wrapper .cover-body .subtitle {
  font-size: 20px;
}
.cover-wrapper .cover-body .logo {
  max-height: 120px;
  max-width: calc(100% - 4 * 16px);
}
@media screen and (min-height: 1024px) {
  .cover-wrapper .cover-body .title {
    font-size: 3rem;
  }
  .cover-wrapper .cover-body .subtitle {
    font-size: 1.05rem;
  }
  .cover-wrapper .cover-body .logo {
    max-height: 150px;
  }
}
.cover-wrapper .cover-body .m_search {
  position: relative;
  max-width: calc(100% - 16px);
  width: 320px;
  vertical-align: middle;
}
.cover-wrapper .cover-body .m_search .form {
  position: relative;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
  width: 100%;
}
.cover-wrapper .cover-body .m_search .icon,
.cover-wrapper .cover-body .m_search .input {
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
}
.cover-wrapper .cover-body .m_search .icon {
  position: absolute;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
  line-height: 2.5rem;
  width: 32px;
  top: 0;
  left: 5px;
  color: rgba(68,68,68,0.75);
}
.cover-wrapper .cover-body .m_search .input {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
  height: 2.5rem;
  width: 100%;
  box-shadow: none;
  -webkit-box-shadow: none;
  box-sizing: border-box;
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  font-size: 0.875rem;
  -webkit-appearance: none;
  padding-left: 36px;
  border-radius: 1.4rem;
  -webkit-border-radius: 1.4rem;
  background: rgba(255,255,255,0.6);
  backdrop-filter: blur(10px);
  border: none;
  color: var(--color-text);
}
@media screen and (max-width: 500px) {
  .cover-wrapper .cover-body .m_search .input {
    padding-left: 36px;
  }
}
.cover-wrapper .cover-body .m_search .input:hover {
  background: rgba(255,255,255,0.8);
}
.cover-wrapper .cover-body .m_search .input:focus {
  background: #fff;
}
.cover-wrapper .list-h {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  -webkit-box-direction: normal;
  -moz-box-direction: normal;
  -webkit-box-orient: horizontal;
  -moz-box-orient: horizontal;
  -webkit-flex-direction: row;
  -ms-flex-direction: row;
  flex-direction: row;
  flex-wrap: wrap;
  -webkit-flex-wrap: wrap;
  -khtml-flex-wrap: wrap;
  -moz-flex-wrap: wrap;
  -o-flex-wrap: wrap;
  -ms-flex-wrap: wrap;
  align-items: stretch;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
}
.cover-wrapper .list-h a {
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  -webkit-flex: 1 0;
  -ms-flex: 1 0;
  flex: 1 0;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  font-weight: 600;
}
.cover-wrapper .list-h a img {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
  border-radius: 2px;
  -webkit-border-radius: 2px;
  margin: 4px;
  min-width: 40px;
  max-width: 44px;
}
@media screen and (max-width: 768px) {
  .cover-wrapper .list-h a img {
    min-width: 36px;
    max-width: 40px;
  }
}
@media screen and (max-width: 500px) {
  .cover-wrapper .list-h a img {
    margin: 2px 4px;
    min-width: 32px;
    max-width: 36px;
  }
}
@media screen and (max-width: 375px) {
  .cover-wrapper .list-h a img {
    min-width: 28px;
    max-width: 32px;
  }
}
.cover-wrapper {
  max-width: 100%;
}
.cover-wrapper.search .bottom .menu {
  margin-top: 16px;
}
.cover-wrapper.search .bottom .menu .list-h a {
  white-space: nowrap;
  -webkit-box-direction: normal;
  -moz-box-direction: normal;
  -webkit-box-orient: horizontal;
  -moz-box-orient: horizontal;
  -webkit-flex-direction: row;
  -ms-flex-direction: row;
  flex-direction: row;
  align-items: baseline;
  padding: 2px;
  margin: 4px;
  color: var(--color-site-inner);
  opacity: 0.75;
  -webkit-opacity: 0.75;
  -moz-opacity: 0.75;
  text-shadow: 0 1px 2px rgba(0,0,0,0.05);
  border-bottom: 2px solid transparent;
}
.cover-wrapper.search .bottom .menu .list-h a i {
  margin-right: 4px;
}
.cover-wrapper.search .bottom .menu .list-h a p {
  font-size: 0.9375rem;
}
.cover-wrapper.search .bottom .menu .list-h a:hover,
.cover-wrapper.search .bottom .menu .list-h a.active,
.cover-wrapper.search .bottom .menu .list-h a:active {
  opacity: 1;
  -webkit-opacity: 1;
  -moz-opacity: 1;
  border-bottom: 2px solid var(--color-site-inner);
}
.cover-wrapper #parallax-window {
  position: absolute;
  width: 100%;
  height: 100%;
  background: transparent;
}
.parallax-mirror {
  animation-delay: 0s;
  animation-duration: 0.5s;
  animation-fill-mode: forwards;
  animation-timing-function: ease-out;
  animation-name: fadeIn;
}
@-moz-keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-opacity: 0;
    -moz-opacity: 0;
    filter: blur(12px);
  }
  100% {
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-opacity: 0;
    -moz-opacity: 0;
    filter: blur(12px);
  }
  100% {
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@-o-keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-opacity: 0;
    -moz-opacity: 0;
    filter: blur(12px);
  }
  100% {
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-opacity: 0;
    -moz-opacity: 0;
    filter: blur(12px);
  }
  100% {
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@font-face {
  font-family: 'UbuntuMono';
  src: url("https://unpkg.com/volantis-static@0.0.1654736714924/media/fonts/UbuntuMono/UbuntuMono-Regular.ttf");
  font-weight: 'normal';
  font-style: 'normal';
  font-display: swap;
}
@font-face {
  font-family: 'Varela Round';
  src: url("https://unpkg.com/volantis-static@0.0.1654736714924/media/fonts/VarelaRound/VarelaRound-Regular.ttf");
  font-weight: 'normal';
  font-style: 'normal';
  font-display: swap;
}
.l_header {
  position: fixed;
  z-index: 1000;
  top: 0;
  width: 100%;
  height: 64px;
  background: var(--color-card);
  box-shadow: 0 1px 2px 0px rgba(0,0,0,0.1);
  -webkit-box-shadow: 0 1px 2px 0px rgba(0,0,0,0.1);
}
.l_header.auto {
  transition: opacity 0.4s ease;
  -webkit-transition: opacity 0.4s ease;
  -khtml-transition: opacity 0.4s ease;
  -moz-transition: opacity 0.4s ease;
  -o-transition: opacity 0.4s ease;
  -ms-transition: opacity 0.4s ease;
  visibility: hidden;
}
.l_header.auto.show {
  opacity: 1 !important;
  -webkit-opacity: 1 !important;
  -moz-opacity: 1 !important;
  visibility: visible;
}
.l_header .container {
  margin-left: 16px;
  margin-right: 16px;
}
.l_header #wrapper {
  height: 100%;
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
}
.l_header #wrapper .nav-main,
.l_header #wrapper .nav-sub {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  flex-wrap: nowrap;
  -webkit-flex-wrap: nowrap;
  -khtml-flex-wrap: nowrap;
  -moz-flex-wrap: nowrap;
  -o-flex-wrap: nowrap;
  -ms-flex-wrap: nowrap;
  justify-content: space-between;
  -webkit-justify-content: space-between;
  -khtml-justify-content: space-between;
  -moz-justify-content: space-between;
  -o-justify-content: space-between;
  -ms-justify-content: space-between;
  align-items: center;
}
.l_header #wrapper .nav-main {
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
}
.l_header #wrapper.sub .nav-main {
  transform: translateY(-64px);
  -webkit-transform: translateY(-64px);
  -khtml-transform: translateY(-64px);
  -moz-transform: translateY(-64px);
  -o-transform: translateY(-64px);
  -ms-transform: translateY(-64px);
}
.l_header #wrapper .nav-sub {
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
  opacity: 0;
  -webkit-opacity: 0;
  -moz-opacity: 0;
  height: 64px;
  width: calc(100% - 2 * 16px);
  position: absolute;
}
.l_header #wrapper .nav-sub ::-webkit-scrollbar {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: none;
}
@media screen and (min-width: 2048px) {
  .l_header #wrapper .nav-sub {
    max-width: 55vw;
    margin: auto;
  }
}
.l_header #wrapper.sub .nav-sub {
  opacity: 1;
  -webkit-opacity: 1;
  -moz-opacity: 1;
}
.l_header #wrapper .title {
  position: relative;
  color: var(--color-text);
  padding-left: 24px;
  max-height: 64px;
}
.l_header #wrapper .nav-main .title {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  flex-shrink: 0;
  line-height: 64px;
  padding: 0 24px;
  font-size: 1.25rem;
  font-family: "Varela Round", "PingFang SC", "Microsoft YaHei", Helvetica, Arial, Helvetica, monospace;
}
.l_header #wrapper .nav-main .title img {
  height: 64px;
}
.l_header .nav-sub {
  max-width: 1080px;
  margin: auto;
}
.l_header .nav-sub .title {
  font-weight: bold;
  font-family: UbuntuMono, "Varela Round", "PingFang SC", "Microsoft YaHei", Helvetica, Arial, Menlo, Monaco, monospace, sans-serif;
  line-height: 1.2;
  max-height: 64px;
  white-space: normal;
  flex-shrink: 1;
}
.l_header .switcher {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: none;
  line-height: 64px;
  align-items: center;
}
.l_header .switcher .s-toc {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: none;
}
@media screen and (max-width: 768px) {
  .l_header .switcher .s-toc {
    display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
    display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
    display: -ms-flexbox /* TWEENER - IE 10 */;
    display: -webkit-flex /* NEW - Chrome */;
    display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
    display: flex;
  }
}
.l_header .switcher >li {
  height: 48px;
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
  margin: 2px;
}
@media screen and (max-width: 500px) {
  .l_header .switcher >li {
    margin: 0 1px;
    height: 48px;
  }
}
.l_header .switcher >li >a {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  justify-content: center;
  -webkit-justify-content: center;
  -khtml-justify-content: center;
  -moz-justify-content: center;
  -o-justify-content: center;
  -ms-justify-content: center;
  align-items: center;
  width: 48px;
  height: 48px;
  padding: 0.85em 1.1em;
  border-radius: 100px;
  -webkit-border-radius: 100px;
  border: none;
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
  color: #44d7b6;
}
.l_header .switcher >li >a:hover {
  border: none;
}
.l_header .switcher >li >a.active,
.l_header .switcher >li >a:active {
  border: none;
  background: var(--color-site-bg);
}
@media screen and (max-width: 500px) {
  .l_header .switcher >li >a {
    width: 36px;
    height: 48px;
  }
}
.l_header .nav-sub .switcher {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
}
.l_header .m_search {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  height: 64px;
  width: 240px;
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
}
@media screen and (max-width: 1024px) {
  .l_header .m_search {
    width: 44px;
    min-width: 44px;
  }
  .l_header .m_search input::placeholder {
    opacity: 0;
    -webkit-opacity: 0;
    -moz-opacity: 0;
  }
  .l_header .m_search:hover {
    width: 240px;
  }
  .l_header .m_search:hover input::placeholder {
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@media screen and (min-width: 500px) {
  .l_header .m_search:hover .input {
    width: 100%;
  }
  .l_header .m_search:hover .input::placeholder {
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
@media screen and (max-width: 500px) {
  .l_header .m_search {
    min-width: 0;
  }
  .l_header .m_search input::placeholder {
    opacity: 1;
    -webkit-opacity: 1;
    -moz-opacity: 1;
  }
}
.l_header .m_search .form {
  position: relative;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  width: 100%;
  align-items: center;
}
.l_header .m_search .icon {
  position: absolute;
  width: 36px;
  left: 5px;
  color: var(--color-meta);
}
@media screen and (max-width: 500px) {
  .l_header .m_search .icon {
    display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
    display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
    display: none;
  }
}
.l_header .m_search .input {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
  padding-top: 8px;
  padding-bottom: 8px;
  line-height: 1.3;
  width: 100%;
  color: var(--color-text);
  background: #fafafa;
  box-shadow: none;
  -webkit-box-shadow: none;
  box-sizing: border-box;
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  padding-left: 40px;
  font-size: 0.875rem;
  border-radius: 8px;
  -webkit-border-radius: 8px;
  border: none;
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
}
@media screen and (min-width: 500px) {
  .l_header .m_search .input:focus {
    box-shadow: 0 4px 8px 0px rgba(0,0,0,0.1);
    -webkit-box-shadow: 0 4px 8px 0px rgba(0,0,0,0.1);
  }
}
@media screen and (max-width: 500px) {
  .l_header .m_search .input {
    background: var(--color-block);
    padding-left: 8px;
    border: none;
  }
  .l_header .m_search .input:hover,
  .l_header .m_search .input:focus {
    border: none;
  }
}
@media (max-width: 500px) {
  .l_header .m_search {
    left: 0;
    width: 0;
    overflow: hidden;
    position: absolute;
    background: #fff;
    transition: all 0.28s ease;
    -webkit-transition: all 0.28s ease;
    -khtml-transition: all 0.28s ease;
    -moz-transition: all 0.28s ease;
    -o-transition: all 0.28s ease;
    -ms-transition: all 0.28s ease;
  }
  .l_header .m_search .input {
    border-radius: 32px;
    -webkit-border-radius: 32px;
    margin-left: 16px;
    padding-left: 16px;
  }
  .l_header.z_search-open .m_search {
    width: 100%;
  }
  .l_header.z_search-open .m_search .input {
    width: calc(100% - 120px);
  }
}
ul.m-pc >li>a {
  color: inherit;
  border-bottom: 2px solid transparent;
}
ul.m-pc >li>a:active,
ul.m-pc >li>a.active {
  border-bottom: 2px solid #44d7b6;
}
ul.m-pc li:hover >ul.list-v,
ul.list-v li:hover >ul.list-v {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
}
ul.nav-list-h {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: -ms-flexbox /* TWEENER - IE 10 */;
  display: -webkit-flex /* NEW - Chrome */;
  display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
  display: flex;
  align-items: stretch;
}
ul.nav-list-h>li {
  position: relative;
  justify-content: center;
  -webkit-justify-content: center;
  -khtml-justify-content: center;
  -moz-justify-content: center;
  -o-justify-content: center;
  -ms-justify-content: center;
  height: 100%;
  line-height: 2.4;
  border-radius: 4px;
  -webkit-border-radius: 4px;
}
ul.nav-list-h>li >a {
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  font-weight: 600;
}
ul.list-v {
  z-index: 1;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: none;
  position: absolute;
  background: var(--color-card);
  box-shadow: 0 2px 4px 0px rgba(0,0,0,0.08), 0 4px 8px 0px rgba(0,0,0,0.08), 0 8px 16px 0px rgba(0,0,0,0.08);
  -webkit-box-shadow: 0 2px 4px 0px rgba(0,0,0,0.08), 0 4px 8px 0px rgba(0,0,0,0.08), 0 8px 16px 0px rgba(0,0,0,0.08);
  margin-top: -6px;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  padding: 8px 0;
}
ul.list-v.show {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
}
ul.list-v hr {
  margin-top: 8px;
  margin-bottom: 8px;
}
ul.list-v >li {
  white-space: nowrap;
  word-break: keep-all;
}
ul.list-v >li.header {
  font-size: 0.78125rem;
  font-weight: bold;
  line-height: 2em;
  color: var(--color-meta);
  margin: 8px 16px 4px;
}
ul.list-v >li.header i {
  margin-right: 8px;
}
ul.list-v >li ul {
  margin-left: 0;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: none;
  margin-top: -40px;
}
ul.list-v .aplayer-container {
  min-height: 64px;
  padding: 6px 16px;
}
ul.list-v >li>a {
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
  color: var(--color-list);
  font-size: 0.875rem;
  font-weight: bold;
  line-height: 36px;
  padding: 0 20px 0 16px;
  text-overflow: ellipsis;
  margin: 0 4px;
  border-radius: 4px;
  -webkit-border-radius: 4px;
}
@media screen and (max-width: 1024px) {
  ul.list-v >li>a {
    line-height: 40px;
  }
}
ul.list-v >li>a >i {
  margin-right: 8px;
}
ul.list-v >li>a:active,
ul.list-v >li>a.active {
  color: var(--color-list-hl);
}
ul.list-v >li>a:hover {
  color: var(--color-list-hl);
  background: var(--color-site-bg);
}
.l_header .menu >ul>li>a {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: block;
  padding: 0 8px;
}
.l_header .menu >ul>li>a >i {
  margin-right: 4px;
}
.l_header ul.nav-list-h>li {
  color: var(--color-list);
  line-height: 64px;
}
.l_header ul.nav-list-h>li >a {
  max-height: 64px;
  overflow: hidden;
  color: inherit;
}
.l_header ul.nav-list-h>li >a:active,
.l_header ul.nav-list-h>li >a.active {
  color: #44d7b6;
}
.l_header ul.nav-list-h>li:hover>a {
  color: var(--color-list-hl);
}
.l_header ul.nav-list-h>li i.music {
  animation: rotate-effect 1.5s linear infinite;
  -webkit-animation: rotate-effect 1.5s linear infinite;
  -khtml-animation: rotate-effect 1.5s linear infinite;
  -moz-animation: rotate-effect 1.5s linear infinite;
  -o-animation: rotate-effect 1.5s linear infinite;
  -ms-animation: rotate-effect 1.5s linear infinite;
}
@-moz-keyframes rotate-effect {
  0% {
    transform: rotate(0);
    -webkit-transform: rotate(0);
    -khtml-transform: rotate(0);
    -moz-transform: rotate(0);
    -o-transform: rotate(0);
    -ms-transform: rotate(0);
  }
  25% {
    transform: rotate(90deg);
    -webkit-transform: rotate(90deg);
    -khtml-transform: rotate(90deg);
    -moz-transform: rotate(90deg);
    -o-transform: rotate(90deg);
    -ms-transform: rotate(90deg);
  }
  50% {
    transform: rotate(180deg);
    -webkit-transform: rotate(180deg);
    -khtml-transform: rotate(180deg);
    -moz-transform: rotate(180deg);
    -o-transform: rotate(180deg);
    -ms-transform: rotate(180deg);
  }
  75% {
    transform: rotate(270deg);
    -webkit-transform: rotate(270deg);
    -khtml-transform: rotate(270deg);
    -moz-transform: rotate(270deg);
    -o-transform: rotate(270deg);
    -ms-transform: rotate(270deg);
  }
  100% {
    transform: rotate(360deg);
    -webkit-transform: rotate(360deg);
    -khtml-transform: rotate(360deg);
    -moz-transform: rotate(360deg);
    -o-transform: rotate(360deg);
    -ms-transform: rotate(360deg);
  }
}
@-webkit-keyframes rotate-effect {
  0% {
    transform: rotate(0);
    -webkit-transform: rotate(0);
    -khtml-transform: rotate(0);
    -moz-transform: rotate(0);
    -o-transform: rotate(0);
    -ms-transform: rotate(0);
  }
  25% {
    transform: rotate(90deg);
    -webkit-transform: rotate(90deg);
    -khtml-transform: rotate(90deg);
    -moz-transform: rotate(90deg);
    -o-transform: rotate(90deg);
    -ms-transform: rotate(90deg);
  }
  50% {
    transform: rotate(180deg);
    -webkit-transform: rotate(180deg);
    -khtml-transform: rotate(180deg);
    -moz-transform: rotate(180deg);
    -o-transform: rotate(180deg);
    -ms-transform: rotate(180deg);
  }
  75% {
    transform: rotate(270deg);
    -webkit-transform: rotate(270deg);
    -khtml-transform: rotate(270deg);
    -moz-transform: rotate(270deg);
    -o-transform: rotate(270deg);
    -ms-transform: rotate(270deg);
  }
  100% {
    transform: rotate(360deg);
    -webkit-transform: rotate(360deg);
    -khtml-transform: rotate(360deg);
    -moz-transform: rotate(360deg);
    -o-transform: rotate(360deg);
    -ms-transform: rotate(360deg);
  }
}
@-o-keyframes rotate-effect {
  0% {
    transform: rotate(0);
    -webkit-transform: rotate(0);
    -khtml-transform: rotate(0);
    -moz-transform: rotate(0);
    -o-transform: rotate(0);
    -ms-transform: rotate(0);
  }
  25% {
    transform: rotate(90deg);
    -webkit-transform: rotate(90deg);
    -khtml-transform: rotate(90deg);
    -moz-transform: rotate(90deg);
    -o-transform: rotate(90deg);
    -ms-transform: rotate(90deg);
  }
  50% {
    transform: rotate(180deg);
    -webkit-transform: rotate(180deg);
    -khtml-transform: rotate(180deg);
    -moz-transform: rotate(180deg);
    -o-transform: rotate(180deg);
    -ms-transform: rotate(180deg);
  }
  75% {
    transform: rotate(270deg);
    -webkit-transform: rotate(270deg);
    -khtml-transform: rotate(270deg);
    -moz-transform: rotate(270deg);
    -o-transform: rotate(270deg);
    -ms-transform: rotate(270deg);
  }
  100% {
    transform: rotate(360deg);
    -webkit-transform: rotate(360deg);
    -khtml-transform: rotate(360deg);
    -moz-transform: rotate(360deg);
    -o-transform: rotate(360deg);
    -ms-transform: rotate(360deg);
  }
}
@keyframes rotate-effect {
  0% {
    transform: rotate(0);
    -webkit-transform: rotate(0);
    -khtml-transform: rotate(0);
    -moz-transform: rotate(0);
    -o-transform: rotate(0);
    -ms-transform: rotate(0);
  }
  25% {
    transform: rotate(90deg);
    -webkit-transform: rotate(90deg);
    -khtml-transform: rotate(90deg);
    -moz-transform: rotate(90deg);
    -o-transform: rotate(90deg);
    -ms-transform: rotate(90deg);
  }
  50% {
    transform: rotate(180deg);
    -webkit-transform: rotate(180deg);
    -khtml-transform: rotate(180deg);
    -moz-transform: rotate(180deg);
    -o-transform: rotate(180deg);
    -ms-transform: rotate(180deg);
  }
  75% {
    transform: rotate(270deg);
    -webkit-transform: rotate(270deg);
    -khtml-transform: rotate(270deg);
    -moz-transform: rotate(270deg);
    -o-transform: rotate(270deg);
    -ms-transform: rotate(270deg);
  }
  100% {
    transform: rotate(360deg);
    -webkit-transform: rotate(360deg);
    -khtml-transform: rotate(360deg);
    -moz-transform: rotate(360deg);
    -o-transform: rotate(360deg);
    -ms-transform: rotate(360deg);
  }
}
.menu-phone li ul.list-v {
  right: calc(100% - 0.5 * 16px);
}
.menu-phone li ul.list-v ul {
  right: calc(100% - 0.5 * 16px);
}
#wrapper {
  max-width: 1080px;
  margin: auto;
}
@media screen and (min-width: 2048px) {
  #wrapper {
    max-width: 55vw;
  }
}
#wrapper .menu {
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  -webkit-flex: 1 1;
  -ms-flex: 1 1;
  flex: 1 1;
  margin: 0 16px 0 0;
}
#wrapper .menu .list-v ul {
  left: calc(100% - 0.5 * 16px);
}
.menu-phone {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: none;
  margin-top: 16px;
  right: 8px;
  transition: all 0.28s ease;
  -webkit-transition: all 0.28s ease;
  -khtml-transition: all 0.28s ease;
  -moz-transition: all 0.28s ease;
  -o-transition: all 0.28s ease;
  -ms-transition: all 0.28s ease;
}
.menu-phone ul {
  right: calc(100% - 0.5 * 16px);
}
@media screen and (max-width: 500px) {
  .menu-phone {
    display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
    display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
    display: block;
  }
}
.l_header {
  max-width: 65vw;
  left: calc((100% - 65vw) * 0.5);
  border-bottom-left-radius: 8px;
  border-bottom-right-radius: 8px;
}
@media screen and (max-width: 2048px) {
  .l_header {
    max-width: 1112px;
    left: calc((100% - 1112px) * 0.5);
  }
}
@media screen and (max-width: 1112px) {
  .l_header {
    left: 0;
    border-radius: 0;
    -webkit-border-radius: 0;
    max-width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .l_header .container {
    margin-left: 0;
    margin-right: 0;
  }
  .l_header #wrapper .nav-main .title {
    padding-left: 16px;
    padding-right: 16px;
  }
  .l_header #wrapper .nav-sub {
    width: 100%;
  }
  .l_header #wrapper .nav-sub .title {
    overflow-y: scroll;
    margin-top: 2px;
    padding: 8px 16px;
  }
  .l_header #wrapper .switcher {
    display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
    display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
    display: -ms-flexbox /* TWEENER - IE 10 */;
    display: -webkit-flex /* NEW - Chrome */;
    display: flex /* NEW, Spec - Opera 12.1, Firefox 20+ */;
    display: flex;
    margin-right: 8px;
  }
  .l_header .menu {
    display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
    display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
    display: none;
  }
}
@media screen and (max-width: 500px) {
  .list-v li {
    max-width: 270px;
  }
}
#u-search {
  display: -webkit-box /* OLD - iOS 6-, Safari 3.1-6 */;
  display: -moz-box /* OLD - Firefox 19- (buggy but mostly works) */;
  display: none;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  padding: 60px 20px;
  z-index: 1001;
}
@media screen and (max-width: 680px) {
  #u-search {
    padding: 0px;
  }
}

  </style>
  <link rel="stylesheet" href="/css/style.css" media="print" onload="this.media='all';this.onload=null">
  <noscript><link rel="stylesheet" href="/css/style.css"></noscript>
  
<script>
if (/*@cc_on!@*/false || (!!window.MSInputMethodContext && !!document.documentMode))
    document.write(
	'<style>'+
		'html{'+
			'overflow-x: hidden !important;'+
			'overflow-y: hidden !important;'+
		'}'+
		'.kill-ie{'+
			'text-align:center;'+
			'height: 100%;'+
			'margin-top: 15%;'+
			'margin-bottom: 5500%;'+
		'}'+
    '.kill-t{'+
      'font-size: 2rem;'+
    '}'+
    '.kill-c{'+
      'font-size: 1.2rem;'+
    '}'+
		'#l_header,#l_body{'+
			'display: none;'+
		'}'+
	'</style>'+
    '<div class="kill-ie">'+
        `<span class="kill-t"><b>Sorry, your browser cannot access this site</b></span><br/>`+
        `<span class="kill-c">Microsoft has terminated support for Internet Explorer (IE) 10 and earlier versions in 2016. <br/>There are great security risks to continue using it. Please use contemporary mainstream browsers to access.</span><br/>`+
        `<a target="_blank" rel="noopener" href="https://blogs.windows.com/windowsexperience/2021/05/19/the-future-of-internet-explorer-on-windows-10-is-in-microsoft-edge/"><strong>Learn more ></strong></a>`+
    '</div>');
</script>


<noscript>
	<style>
		html{
			overflow-x: hidden !important;
			overflow-y: hidden !important;
		}
		.kill-noscript{
			text-align:center;
			height: 100%;
			margin-top: 15%;
			margin-bottom: 5500%;
		}
    .kill-t{
      font-size: 2rem;
    }
    .kill-c{
      font-size: 1.2rem;
    }
		#l_header,#l_body{
			display: none;
		}
	</style>
    <div class="kill-noscript">
        <span class="kill-t"><b>Sorry, your browser cannot access this site</b></span><br/>
        <span class="kill-c">This page requires browser support (enable) JavaScript</span><br/>
        <a target="_blank" rel="noopener" href="https://www.baidu.com/s?wd=启用JavaScript"><strong>Learn more ></strong></a>
    </div>
</noscript>


  <script>
  /************这个文件存放不需要重载的全局变量和全局函数*********/
  window.volantis = {}; // volantis 全局变量
  volantis.debug = "env"; // 调试模式
  volantis.dom = {}; // 页面Dom see: /source/js/app.js etc.

  volantis.GLOBAL_CONFIG ={
    debug: "env",
    cdn: {"js":{"app":"/js/app.js","parallax":"/js/plugins/parallax.js","rightMenu":"/js/plugins/rightMenu.js","rightMenus":"/js/plugins/rightMenus.js","sites":"/js/plugins/tags/sites.js","friends":"/js/plugins/tags/friends.js","contributors":"/js/plugins/tags/contributors.js","search":"/js/search/hexo.js"},"css":{"style":"/css/style.css"}},
    default: {"avatar":"https://unpkg.com/volantis-static@0.0.1654736714924/media/placeholder/avatar/round/3442075.svg","link":"https://unpkg.com/volantis-static@0.0.1654736714924/media/placeholder/link/8f277b4ee0ecd.svg","cover":"https://unpkg.com/volantis-static@0.0.1654736714924/media/placeholder/cover/76b86c0226ffd.svg","image":"https://unpkg.com/volantis-static@0.0.1654736714924/media/placeholder/image/2659360.svg"},
    lastupdate: new Date(1765549629403),
    sidebar: {
      for_page: ["blogger","category","tagcloud","webinfo","lastupdate"],
      for_post: ["toc"],
      webinfo: {
        lastupd: {
          enable: true,
          friendlyShow: true
        },
        runtime: {
          data: "2022/12/04",
          unit: "天"
        }
      }
    },
    plugins: {
      message: {"enable":true,"css":"https://unpkg.com/volantis-static@0.0.1654736714924/libs/izitoast/dist/css/iziToast.min.css","js":"https://unpkg.com/volantis-static@0.0.1654736714924/libs/izitoast/dist/js/iziToast.min.js","icon":{"default":"fa-solid fa-info-circle light-blue","quection":"fa-solid fa-question-circle light-blue"},"time":{"default":5000,"quection":20000},"position":"topRight","transitionIn":"bounceInLeft","transitionOut":"fadeOutRight","titleColor":"var(--color-text)","messageColor":"var(--color-text)","backgroundColor":"var(--color-card)","zindex":2147483647,"copyright":{"enable":true,"title":"知识共享许可协议","message":"请遵守 CC BY-NC-SA 4.0 协议。","icon":"far fa-copyright light-blue"},"aplayer":{"enable":true,"play":"fa-solid fa-play","pause":"fa-solid fa-pause"},"rightmenu":{"enable":true,"notice":true}},
      fancybox: {"css":"https://unpkg.com/volantis-static@0.0.1654736714924/libs/@fancyapps/ui/dist/fancybox.css","js":"https://unpkg.com/volantis-static@0.0.1654736714924/libs/@fancyapps/ui/dist/fancybox.umd.js"},
      
      aplayer: {
        id: 993524571,
        enable:true
      },
      
      
      
    }
  }

  /******************** volantis.EventListener ********************************/
  // 事件监听器 see: /source/js/app.js
  volantis.EventListener = {}
  // 这里存放pjax切换页面时将被移除的事件监听器
  volantis.EventListener.list = []
  //构造方法
  function volantisEventListener(type, f, ele) {
    this.type = type
    this.f = f
    this.ele = ele
  }
  // 移除事件监听器
  volantis.EventListener.remove = () => {
    volantis.EventListener.list.forEach(function (i) {
      i.ele.removeEventListener(i.type, i.f, false)
    })
    volantis.EventListener.list = []
  }
  /******************** volantis.dom.$ ********************************/
  // 注：这里没有选择器，也没有forEach一次只处理一个dom，这里重新封装主题常用的dom方法，返回的是dom对象，对象包含了以下方法，同时保留dom的原生API
  function volantisDom(ele) {
    if (!ele) ele = document.createElement("div")
    this.ele = ele;
    // ==============================================================
    this.ele.find = (c) => {
      let q = this.ele.querySelector(c)
      if (q)
        return new volantisDom(q)
    }
    // ==============================================================
    this.ele.hasClass = (c) => {
      return this.ele.className.match(new RegExp('(\\s|^)' + c + '(\\s|$)'));
    }
    this.ele.addClass = (c) => {
      this.ele.classList.add(c);
      return this.ele
    }
    this.ele.removeClass = (c) => {
      this.ele.classList.remove(c);
      return this.ele
    }
    this.ele.toggleClass = (c) => {
      if (this.ele.hasClass(c)) {
        this.ele.removeClass(c)
      } else {
        this.ele.addClass(c)
      }
      return this.ele
    }
    // ==============================================================
    // 参数 r 为 true 表示pjax切换页面时事件监听器将被移除，false不移除
    this.ele.on = (c, f, r = 1) => {
      this.ele.addEventListener(c, f, false)
      if (r) {
        volantis.EventListener.list.push(new volantisEventListener(c, f, this.ele))
      }
      return this.ele
    }
    this.ele.click = (f, r) => {
      this.ele.on("click", f, r)
      return this.ele
    }
    this.ele.scroll = (f, r) => {
      this.ele.on("scroll", f, r)
      return this.ele
    }
    // ==============================================================
    this.ele.html = (c) => {
      // if(c=== undefined){
      //   return this.ele.innerHTML
      // }else{
      this.ele.innerHTML = c
      return this.ele
      // }
    }
    // ==============================================================
    this.ele.hide = (c) => {
      this.ele.style.display = "none"
      return this.ele
    }
    this.ele.show = (c) => {
      this.ele.style.display = "block"
      return this.ele
    }
    // ==============================================================
    return this.ele
  }
  volantis.dom.$ = (ele) => {
    return !!ele ? new volantisDom(ele) : null;
  }
  /******************** RunItem ********************************/
  function RunItem() {
    this.list = []; // 存放回调函数
    this.start = () => {
      for (var i = 0; i < this.list.length; i++) {
        this.list[i].run();
      }
    };
    this.push = (fn, name, setRequestAnimationFrame = true) => {
      let myfn = fn
      if (setRequestAnimationFrame) {
        myfn = ()=>{
          volantis.requestAnimationFrame(fn)
        }
      }
      var f = new Item(myfn, name);
      this.list.push(f);
    };
    this.remove = (name) =>{
      for (let index = 0; index < this.list.length; index++) {
        const e = this.list[index];
        if (e.name == name) {
          this.list.splice(index,1);
        }
      }
    }
    // 构造一个可以run的对象
    function Item(fn, name) {
      // 函数名称
      this.name = name || fn.name;
      // run方法
      this.run = () => {
        try {
          fn()
        } catch (error) {
          console.log(error);
        }
      };
    }
  }
  /******************** Pjax ********************************/
  // /layout/_plugins/pjax/index.ejs
  // volantis.pjax.send(callBack[,"callBackName"]) 传入pjax:send回调函数
  // volantis.pjax.push(callBack[,"callBackName"]) 传入pjax:complete回调函数
  // volantis.pjax.error(callBack[,"callBackName"]) 传入pjax:error回调函数
  volantis.pjax = {};
  volantis.pjax.method = {
    complete: new RunItem(),
    error: new RunItem(),
    send: new RunItem(),
  };
  volantis.pjax = Object.assign(volantis.pjax, {
    push: volantis.pjax.method.complete.push,
    error: volantis.pjax.method.error.push,
    send: volantis.pjax.method.send.push,
  });
  /******************** RightMenu ********************************/
  // volantis.rightmenu.handle(callBack[,"callBackName"]) 外部菜单项控制
  // 可在 volantis.mouseEvent 处获取右键事件
  volantis.rightmenu = {};
  volantis.rightmenu.method = {
    handle: new RunItem(),
  }
  volantis.rightmenu = Object.assign(volantis.rightmenu, {
    handle: volantis.rightmenu.method.handle.push,
  });
  /********************  Dark Mode  ********************************/
  // /layout/_partial/scripts/darkmode.ejs
  // volantis.dark.mode 当前模式 dark or light
  // volantis.dark.toggle() 暗黑模式触发器
  // volantis.dark.push(callBack[,"callBackName"]) 传入触发器回调函数
  volantis.dark = {};
  volantis.dark.method = {
    toggle: new RunItem(),
  };
  volantis.dark = Object.assign(volantis.dark, {
    push: volantis.dark.method.toggle.push,
  });
  /********************  Message  ********************************/
  // VolantisApp.message
  /********************  isMobile  ********************************/
  // /source/js/app.js
  // volantis.isMobile
  // volantis.isMobileOld
  /********************脚本动态加载函数********************************/
  // volantis.js(src, cb)  cb 可以传入onload回调函数 或者 JSON对象 例如: volantis.js("src", ()=>{}) 或 volantis.js("src", {defer:true,onload:()=>{}})
  // volantis.css(src)

  // 返回Promise对象，如下方法同步加载资源，这利于处理文件资源之间的依赖关系，例如：APlayer 需要在 MetingJS 之前加载
  // (async () => {
  //     await volantis.js("...theme.plugins.aplayer.js.aplayer...")
  //     await volantis.js("...theme.plugins.aplayer.js.meting...")
  // })();

  // 已经加入了setTimeout
  volantis.js = (src, cb) => {
    return new Promise(resolve => {
      setTimeout(function () {
        var HEAD = document.getElementsByTagName("head")[0] || document.documentElement;
        var script = document.createElement("script");
        script.setAttribute("type", "text/javascript");
        if (cb) {
          if (JSON.stringify(cb)) {
            for (let p in cb) {
              if (p == "onload") {
                script[p] = () => {
                  cb[p]()
                  resolve()
                }
              } else {
                script[p] = cb[p]
                script.onload = resolve
              }
            }
          } else {
            script.onload = () => {
              cb()
              resolve()
            };
          }
        } else {
          script.onload = resolve
        }
        script.setAttribute("src", src);
        HEAD.appendChild(script);
      });
    });
  }
  volantis.css = (src) => {
    return new Promise(resolve => {
      setTimeout(function () {
        var link = document.createElement('link');
        link.rel = "stylesheet";
        link.href = src;
        link.onload = resolve;
        document.getElementsByTagName("head")[0].appendChild(link);
      });
    });
  }
  /********************按需加载的插件********************************/
  // volantis.import.jQuery().then(()=>{})
  volantis.import = {
    jQuery: () => {
      if (typeof jQuery == "undefined") {
        return volantis.js("https://unpkg.com/volantis-static@0.0.1654736714924/libs/jquery/dist/jquery.min.js")
      } else {
        return new Promise(resolve => {
          resolve()
        });
      }
    }
  }
  /********************** requestAnimationFrame ********************************/
  // 1、requestAnimationFrame 会把每一帧中的所有 DOM 操作集中起来，在一次重绘或回流中就完成，并且重绘或回流的时间间隔紧紧跟随浏览器的刷新频率，一般来说，这个频率为每秒60帧。
  // 2、在隐藏或不可见的元素中，requestAnimationFrame 将不会进行重绘或回流，这当然就意味着更少的的 cpu，gpu 和内存使用量。
  volantis.requestAnimationFrame = (fn)=>{
    if (!window.requestAnimationFrame) {
      window.requestAnimationFrame = window.requestAnimationFrame || window.mozRequestAnimationFrame || window.webkitRequestAnimationFrame;
    }
    window.requestAnimationFrame(fn)
  }
  /************************ layoutHelper *****************************************/
  volantis.layoutHelper = (helper, html, opt)=>{
    opt = Object.assign({clean:false, pjax:true}, opt)
    function myhelper(helper, html, clean) {
      volantis.tempDiv = document.createElement("div");
      volantis.tempDiv.innerHTML = html;
      let layoutHelper = document.querySelector("#layoutHelper-"+helper)
      if (layoutHelper) {
        if (clean) {
          layoutHelper.innerHTML = ""
        }
        layoutHelper.append(volantis.tempDiv);
      }
    }
    myhelper(helper, html, opt.clean)
    if (opt.pjax) {
      volantis.pjax.push(()=>{
        myhelper(helper, html, opt.clean)
      },"layoutHelper-"+helper)
    }
  }
  /****************************** 滚动事件处理 ****************************************/
  volantis.scroll = {
    engine: new RunItem(),
    unengine: new RunItem(),
  };
  volantis.scroll = Object.assign(volantis.scroll, {
    push: volantis.scroll.engine.push,
  });
  // 滚动条距离顶部的距离
  volantis.scroll.getScrollTop = () =>{
    let scrollPos;
    if (window.pageYOffset) {
      scrollPos = window.pageYOffset;
    } else if (document.compatMode && document.compatMode != 'BackCompat') {
      scrollPos = document.documentElement.scrollTop;
    } else if (document.body) {
      scrollPos = document.body.scrollTop;
    }
    return scrollPos;
  }
  // 使用 requestAnimationFrame 处理滚动事件
  // `volantis.scroll.del` 中存储了一个数值, 该数值检测一定时间间隔内滚动条滚动的位移, 数值的检测频率是浏览器的刷新频率. 数值为正数时, 表示向下滚动. 数值为负数时, 表示向上滚动.
  volantis.scroll.handleScrollEvents = () => {
    volantis.scroll.lastScrollTop = volantis.scroll.getScrollTop()
    function loop() {
      const scrollTop = volantis.scroll.getScrollTop();
      if (volantis.scroll.lastScrollTop !== scrollTop) {
        volantis.scroll.del = scrollTop - volantis.scroll.lastScrollTop;
        volantis.scroll.lastScrollTop = scrollTop;
        // if (volantis.scroll.del > 0) {
        //   console.log("向下滚动");
        // } else {
        //   console.log("向上滚动");
        // }
        // 注销过期的unengine未滚动事件
        volantis.scroll.unengine.list=[]
        volantis.scroll.engine.start();
      }else{
        volantis.scroll.unengine.start();
      }
      volantis.requestAnimationFrame(loop)
    }
    volantis.requestAnimationFrame(loop)
  }
  volantis.scroll.handleScrollEvents()
  volantis.scroll.ele = null;
  // 触发页面滚动至目标元素位置
  volantis.scroll.to = (ele, option = {}) => {
    if (!ele) return;
    volantis.scroll.ele = ele;
    // 默认配置
    opt = {
      top: ele.getBoundingClientRect().top + document.documentElement.scrollTop,
      behavior: "smooth"
    }
    // 定义配置
    if ("top" in option) {
      opt.top = option.top
    }
    if ("behavior" in option) {
      opt.behavior = option.behavior
    }
    if ("addTop" in option) {
      opt.top += option.addTop
    }
    if (!("observerDic" in option)) {
      option.observerDic = 100
    }
    // 滚动
    window.scrollTo(opt);
    // 监视器
    // 监视并矫正元素滚动到指定位置
    // 用于处理 lazyload 引起的 cls 导致的定位失败问题
    // option.observer = false
    if (option.observer) {
      setTimeout(() => {
        if (volantis.scroll.ele != ele) {
          return
        }
        volantis.scroll.unengine.push(() => {
          let me = ele.getBoundingClientRect().top
          if(!(me >= -option.observerDic && me <= option.observerDic)){
            volantis.scroll.to(ele, option)
          }
          volantis.scroll.unengine.remove("unengineObserver")
        },"unengineObserver")
      },1000)
    }
  }
  /********************** Content Visibility ********************************/
  // 见 source/css/first.styl 如果遇到任何问题 删除 .post-story 即可
  // 一个元素被声明 content-visibility 属性后 如果元素不在 viewport 中 浏览器不会计算其后代元素样式和属性 从而节省 Style & Layout 耗时
  // content-visibility 的副作用: 锚点失效 等等(实验初期 暂不明确), 使用此方法清除样式
  volantis.cleanContentVisibility = ()=>{
    if (document.querySelector(".post-story")) {
      console.log("cleanContentVisibility");
      document.querySelectorAll(".post-story").forEach(e=>{
        e.classList.remove("post-story")
      })
    }
  }
  /******************************************************************************/
  /******************************************************************************/
  /******************************************************************************/
  //图像加载出错时的处理
  function errorImgAvatar(img) {
    img.src = "https://unpkg.com/volantis-static@0.0.1654736714924/media/placeholder/avatar/round/3442075.svg";
    img.onerror = null;
  }
  function errorImgCover(img) {
    img.src = "https://unpkg.com/volantis-static@0.0.1654736714924/media/placeholder/cover/76b86c0226ffd.svg";
    img.onerror = null;
  }
  /******************************************************************************/
</script>

  <!-- import head_end begin -->
  <!-- import head_end end -->
  <!-- Custom Files headEnd begin-->
  
  <!-- Custom Files headEnd end-->
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://unpkg.com/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://unpkg.com/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>
  <body itemscope itemtype="http://schema.org/WebPage">
    <!-- import body_begin begin-->
    <!-- import body_begin end-->
    <!-- Custom Files bodyBegin begin-->
    
    <!-- Custom Files bodyBegin end-->
    <header itemscope itemtype="http://schema.org/WPHeader" id="l_header" class="l_header auto shadow floatable blur " style='opacity: 0' >
  <div class='container'>
  <div id='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h m-phone' id="pjax-header-nav-list">
        <li><a id="s-comment" class="fa-solid fa-comments fa-fw" target="_self"  href="/" onclick="return false;" title="comment"></a></li>
        
          <li><a id="s-toc" class="s-toc fa-solid fa-list fa-fw" target="_self"  href="/" onclick="return false;" title="toc"></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
            <img no-lazy class='logo' src='https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/Logo-NavBar@3x.png'/>
          
          
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h m-pc'>
          
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/" title="博客"
                  
                  
                  
                    active-action="action-home"
                  >
                  <i class='fa-solid fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/categories/" title="分类"
                  
                  
                  
                    active-action="action-categories"
                  >
                  <i class='fa-solid fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/tags/" title="标签"
                  
                  
                  
                    active-action="action-tags"
                  >
                  <i class='fa-solid fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/archives/" title="归档"
                  
                  
                  
                    active-action="action-archives"
                  >
                  <i class='fa-solid fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/friends/" title="友链"
                  
                  
                  
                    active-action="action-friends"
                  >
                  <i class='fa-solid fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/about/" title="关于"
                  
                  
                  
                    active-action="action-about"
                  >
                  <i class='fa-solid fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
				</ul>
			</div>
      
      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fa-solid fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>
      

			<ul class='switcher nav-list-h m-phone'>
				
					<li><a class="s-search fa-solid fa-search fa-fw" target="_self" href="/" onclick="return false;" title="search"></a></li>
				
				<li>
          <a class="s-menu fa-solid fa-bars fa-fw" target="_self" href="/" onclick="return false;" title="menu"></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/" title="博客"
                  
                  
                  
                    active-action="action-home"
                  >
                  <i class='fa-solid fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/categories/" title="分类"
                  
                  
                  
                    active-action="action-categories"
                  >
                  <i class='fa-solid fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/tags/" title="标签"
                  
                  
                  
                    active-action="action-tags"
                  >
                  <i class='fa-solid fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/archives/" title="归档"
                  
                  
                  
                    active-action="action-archives"
                  >
                  <i class='fa-solid fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/friends/" title="友链"
                  
                  
                  
                    active-action="action-friends"
                  >
                  <i class='fa-solid fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover"
                href="/about/" title="关于"
                  
                  
                  
                    active-action="action-about"
                  >
                  <i class='fa-solid fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
          </ul>
        </li>
			</ul>

      <!-- Custom Files header begin -->
      
      <!-- Custom Files header end -->
		</div>
	</div>
  </div>
</header>

    <div id="l_body">
      <div id="l_cover">
  
    
      <!-- see: /layout/_partial/scripts/_ctrl/coverCtrl.ejs -->
      <div id="half" class='cover-wrapper post search' style="display: ;">
        
  <div id="parallax-window"></div>

<div class='cover-body'>
  <div class='top'>
    
    
      <p class="title">MengFanjun的博客</p>
    
    
  </div>
  <div class='bottom'>
    
      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <input type="text" class="input u-search-input" placeholder="搜一下" />
          <i class="icon fa-solid fa-search fa-fw"></i>
        </form>
      </div>
    
    <div class='menu navigation'>
      <div class='list-h'>
        
          
            <a href="/"
              
              
              active-action="action-home">
              <p>主页</p>
            </a>
          
            <a href="/tags/"
              
              
              active-action="action-tags">
              <p>标签</p>
            </a>
          
        
      </div>
    </div>
  </div>
</div>

        <div id="scroll-down" style="display: none;"><i class="fa fa-chevron-down scroll-down-effects"></i></div>
      </div>
    
  
</div>

      <div id="safearea">
        <div class="body-wrapper">
          
<div id="l_main" class=''>
  <article itemscope itemtype="http://schema.org/Article" class="article post white-box reveal md shadow floatable blur article-type-post" id="post" itemscope itemprop="blogPost">
  <link itemprop="mainEntityOfPage" href="https://mengfanjun020906.github.io/2025/08/30/Standford CS336（二）训练模型介绍/">
  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="MengFanjun的博客">
  </span>
  <span hidden itemprop="post" itemscope itemtype="http://schema.org/Post">
    <meta itemprop="name" content="MengFanjun的博客">
    <meta itemprop="description" content="20级通信工程大学生的个人博客">
  </span>
  


  
    <span hidden>
      <meta itemprop="image" content="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png">
    </span>
  
  <div class="article-meta" id="top">
    
    
    
      <h1 class="title" itemprop="name headline">
        Standford CS336（二）训练模型介绍
      </h1>
      <div class='new-meta-box'>
        
          
            
<div class='new-meta-item author' itemprop="author" itemscope itemtype="http://schema.org/Person">
  <a itemprop="url" class='author' href="/" rel="nofollow">
    <img itemprop="image" src="https://pic.imgdb.cn/item/63f9926ef144a01007de7d7d.png" class="lazyload" data-srcset="https://pic.imgdb.cn/item/63f9926ef144a01007de7d7d.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">
    <p itemprop="name">MengFanjun</p>
  </a>
</div>

          
        
          
            

          
        
          
            <div class="new-meta-item date" itemprop="dateCreated datePublished" datetime="2025-08-30T10:57:21+08:00">
  <a class='notlink'>
    <i class="fa-solid fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：Aug 30, 2025</p>
  </a>
</div>

          
        
          
            
  <div class="new-meta-item wordcount">
    <a class='notlink'>
      <i class="fa-solid fa-keyboard fa-fw" aria-hidden="true"></i>
      <p>7.1k words</p>
    </a>
  </div>
  <div class="new-meta-item readtime">
    <a class='notlink'>
      <i class="fa-solid fa-hourglass-half fa-fw" aria-hidden="true"></i>
      <p>34 min</p>
    </a>
  </div>


          
        
          
            


<div class="new-meta-item browse">
  <a class='notlink'>
    <p>
      <i class="fa-solid fa-eye fa-fw" aria-hidden="true"></i>
      
      <span id="busuanzi_value_page_pv"><i class="fa-solid fa-loader fa-spin fa-fw" aria-hidden="true"></i></span>
      
      <span>次浏览</span>
    </p>
  </a>
</div>


          
        
        <!-- Custom Files topMeta begin-->
        
        <!-- Custom Files topMeta end-->
      </div>
    
  </div>


  <div id="layoutHelper-page-plugins"></div>
  <div id="post-body" itemprop="articleBody">
    <p>github库链接：<a target="_blank" rel="noopener" href="https://github.com/stanford-cs336/spring2025-lectures">https://github.com/stanford-cs336/spring2025-lectures</a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本讲将讨论训练模型所需的所有基本要素，从张量到底层模型，再到优化器和训练循环。我们将密切关注效率（资源利用）。</p>
<h3 id="资源类型"><a href="#资源类型" class="headerlink" title="资源类型"></a>资源类型</h3><ul>
<li>内存 (GB)</li>
<li>计算 (FLOPs)</li>
</ul>
<h2 id="内存核算"><a href="#内存核算" class="headerlink" title="内存核算"></a>内存核算</h2><h3 id="张量基础"><a href="#张量基础" class="headerlink" title="张量基础"></a>张量基础</h3><p>张量是存储所有内容（参数、梯度、优化器状态、数据、激活）的基本构建块。PyTorch 提供了强大的张量操作功能。</p>
<p>PyTorch 中创建张量的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1.</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># 直接从数据创建张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;torch.tensor: <span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">4</span>, <span class="number">8</span>)  <span class="comment"># 4x8 全零矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;torch.zeros (4x8):\n<span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">4</span>, <span class="number">8</span>)  <span class="comment"># 4x8 全一矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;torch.ones (4x8):\n<span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">8</span>)  <span class="comment"># 4x8 独立同分布 Normal(0, 1) 样本矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;torch.randn (4x8):\n<span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>分配但不初始化值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">x = torch.empty(<span class="number">4</span>, <span class="number">8</span>)  <span class="comment"># 4x8 未初始化值矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;torch.empty (4x8):\n<span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用 nn.init 模块进行初始化</span></span><br><span class="line">nn.init.trunc_normal_(x, mean=<span class="number">0</span>, std=<span class="number">1</span>, a=-<span class="number">2</span>, b=<span class="number">2</span>) <span class="comment"># 截断正态分布初始化</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Initialized with trunc_normal_:\n<span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="张量内存"><a href="#张量内存" class="headerlink" title="张量内存"></a>张量内存</h3><p>几乎所有内容都存储为浮点数。理解不同数据类型的内存占用对优化模型至关重要。</p>
<h4 id="float32-单精度"><a href="#float32-单精度" class="headerlink" title="float32 (单精度)"></a>float32 (单精度)</h4><p><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/0751c706e49c41918552daa6d8c68fee.png" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/0751c706e49c41918552daa6d8c68fee.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="在这里插入图片描述"></p>
<ul>
<li>默认数据类型，传统科学计算的基准。</li>
<li>内存使用量由值数量和数据类型决定。每个 float32 占用 4 字节。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">4</span>, <span class="number">8</span>) <span class="comment"># 默认是 float32</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor dtype: <span class="subst">&#123;x.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of elements: <span class="subst">&#123;x.numel()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of each element (bytes): <span class="subst">&#123;x.element_size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total memory usage (bytes): <span class="subst">&#123;x.numel() * x.element_size()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个大型矩阵的内存示例</span></span><br><span class="line"><span class="comment"># 12288 * 4 x 12288 的矩阵大约需要 2.3 GB 内存</span></span><br><span class="line"><span class="comment"># get_memory_usage 函数在原始文件中定义，这里我们直接计算</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Memory for 12288*4 x 12288 matrix (float32): <span class="subst">&#123;<span class="number">12288</span> * <span class="number">4</span> * <span class="number">12288</span> * <span class="number">4</span> / (<span class="number">1024</span>**<span class="number">3</span>):<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="float16-半精度"><a href="#float16-半精度" class="headerlink" title="float16 (半精度)"></a>float16 (半精度)</h4><p><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/8caa4d1990484f2582db83d7a1c3530a.png" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/8caa4d1990484f2582db83d7a1c3530a.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="在这里插入图片描述"></p>
<ul>
<li>内存减半。每个 float16 占用 2 字节。</li>
<li>动态范围（特别是小数字）不佳，可能导致下溢，训练不稳定。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">4</span>, <span class="number">8</span>, dtype=torch.float16)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor dtype: <span class="subst">&#123;x.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of each element (bytes): <span class="subst">&#123;x.element_size()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 演示下溢问题</span></span><br><span class="line">x_underflow = torch.tensor([<span class="number">1e-8</span>], dtype=torch.float16)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;1e-8 in float16: <span class="subst">&#123;x_underflow&#125;</span> (可能为0，表示下溢)&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="bfloat16"><a href="#bfloat16" class="headerlink" title="bfloat16"></a>bfloat16</h4><p><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/dad1981a4c43480eae913451a2ce374e.png" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/dad1981a4c43480eae913451a2ce374e.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="在这里插入图片描述"></p>
<ul>
<li>Google Brain 于 2018 年开发，解决 float16 的下溢问题。</li>
<li>与 float16 内存相同，但指数位与 float32 相同，因此动态范围与 float32 相同。</li>
<li>分辨率较差，但对深度学习影响较小。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1e-8</span>], dtype=torch.bfloat16)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;1e-8 in bfloat16: <span class="subst">&#123;x&#125;</span> (不会下溢)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较不同数据类型的信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Data Type Info ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;float32: <span class="subst">&#123;torch.finfo(torch.float32)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;float16: <span class="subst">&#123;torch.finfo(torch.float16)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;bfloat16: <span class="subst">&#123;torch.finfo(torch.bfloat16)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="fp8"><a href="#fp8" class="headerlink" title="fp8"></a>fp8</h4><p><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/df6d7649a3bdb77cfdc38092d8387a99.png" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/df6d7649a3bdb77cfdc38092d8387a99.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="fp8"></p>
<ul>
<li>2022 年标准化，受机器学习工作负载驱动。</li>
<li>H100 支持两种 FP8 变体：E4M3 (范围 [-448, 448]) 和 E5M2 ([-57344, 57344])。</li>
<li>参考：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.05433.pdf">https://arxiv.org/pdf/2209.05433.pdf</a></li>
</ul>
<h4 id="训练影响"><a href="#训练影响" class="headerlink" title="训练影响"></a>训练影响</h4><ul>
<li>使用 float32 训练可行，但需要大量内存。</li>
<li>使用 fp8、float16 和 bfloat16 训练有风险，可能导致不稳定。</li>
<li>解决方案：混合精度训练（稍后讨论）。</li>
</ul>
<h2 id="计算核算"><a href="#计算核算" class="headerlink" title="计算核算"></a>计算核算</h2><h3 id="GPU-上的张量"><a href="#GPU-上的张量" class="headerlink" title="GPU 上的张量"></a>GPU 上的张量</h3><p>默认情况下，张量存储在 CPU 内存中。为了利用 GPU 的大规模并行性，需要将它们移动到 GPU 内存。</p>
<p><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/f41c21fdbeb2431da6832e80b9f14479.png" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/f41c21fdbeb2431da6832e80b9f14479.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">32</span>, <span class="number">32</span>) <span class="comment"># 默认在 CPU 上</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Initial tensor device: <span class="subst">&#123;x.device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Number of CUDA devices: <span class="subst">&#123;torch.cuda.device_count()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count()):</span><br><span class="line">        properties = torch.cuda.get_device_properties(i)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Device <span class="subst">&#123;i&#125;</span> properties: <span class="subst">&#123;properties.name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将张量移动到 GPU (设备 0)</span></span><br><span class="line">    y = x.to(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Tensor moved to GPU device: <span class="subst">&#123;y.device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接在 GPU 上创建张量</span></span><br><span class="line">    z = torch.zeros(<span class="number">32</span>, <span class="number">32</span>, device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Tensor created directly on GPU device: <span class="subst">&#123;z.device&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA is not available. Cannot move tensors to GPU.&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="张量操作"><a href="#张量操作" class="headerlink" title="张量操作"></a>张量操作</h3><p>大多数张量是通过对其他张量执行操作而创建的。每个操作都有内存和计算后果。</p>
<h4 id="张量存储"><a href="#张量存储" class="headerlink" title="张量存储"></a>张量存储</h4><p>PyTorch 张量是指向已分配内存的指针，带有描述如何获取张量任何元素的元数据。这使得切片和视图操作非常高效。</p>
<p><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/97aa05a6701b46521cb8a7c1e096c7e7.png" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/97aa05a6701b46521cb8a7c1e096c7e7.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="2D Tensor Strides"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([</span><br><span class="line">    [<span class="number">0.</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步长 (stride) 表示访问下一个元素需要跳过的字节数</span></span><br><span class="line"><span class="comment"># 对于二维张量，stride(0) 是访问下一行需要跳过的元素数量</span></span><br><span class="line"><span class="comment"># stride(1) 是访问下一列需要跳过的元素数量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tensor strides: <span class="subst">&#123;x.stride()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Stride for dimension 0 (rows): <span class="subst">&#123;x.stride(<span class="number">0</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Stride for dimension 1 (columns): <span class="subst">&#123;x.stride(<span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如何通过步长计算元素索引</span></span><br><span class="line">r, c = <span class="number">1</span>, <span class="number">2</span> <span class="comment"># 访问第1行第2列的元素 (值为6)</span></span><br><span class="line">index = r * x.stride(<span class="number">0</span>) + c * x.stride(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Calculated index for element (<span class="subst">&#123;r&#125;</span>,<span class="subst">&#123;c&#125;</span>): <span class="subst">&#123;index&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Value at calculated index: <span class="subst">&#123;x.flatten()[index]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="张量切片"><a href="#张量切片" class="headerlink" title="张量切片"></a>张量切片</h4><p>许多操作只是提供张量的不同“视图”，不进行复制。这意味着它们共享底层存储，因此一个张量的修改会影响另一个。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1.</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original tensor x:\n<span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取行 0</span></span><br><span class="line">y_row = x[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;View of row 0 (y_row): <span class="subst">&#123;y_row&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 检查是否共享存储 (需要一个辅助函数，这里简化为概念说明)</span></span><br><span class="line"><span class="comment"># assert same_storage(x, y_row) # 原始文件中的 same_storage 函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取列 1</span></span><br><span class="line">y_col = x[:, <span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;View of column 1 (y_col): <span class="subst">&#123;y_col&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变视图 (view) - 如果存储是连续的，可以改变形状</span></span><br><span class="line">y_view = x.view(<span class="number">3</span>, <span class="number">2</span>) <span class="comment"># 将 2x3 视为 3x2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;View as 3x2 (y_view):\n<span class="subst">&#123;y_view&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转置 (transpose) - 也是视图操作</span></span><br><span class="line">y_transpose = x.transpose(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Transposed view (y_transpose):\n<span class="subst">&#123;y_transpose&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 演示修改对视图的影响</span></span><br><span class="line">x[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x after modification: <span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y_row after x modification: <span class="subst">&#123;y_row&#125;</span>&quot;</span>) <span class="comment"># y_row 也会改变</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y_transpose after x modification: <span class="subst">&#123;y_transpose&#125;</span>&quot;</span>) <span class="comment"># y_transpose 也会改变</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 非连续视图和 contiguous()</span></span><br><span class="line">x_non_contiguous = torch.tensor([[<span class="number">1.</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">y_non_contiguous = x_non_contiguous.transpose(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is y_non_contiguous contiguous? <span class="subst">&#123;y_non_contiguous.is_contiguous()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 强制连续化会创建副本</span></span><br><span class="line">y_contiguous = y_non_contiguous.contiguous().view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is y_contiguous contiguous? <span class="subst">&#123;y_contiguous.is_contiguous()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y_contiguous after contiguous() and view:\n<span class="subst">&#123;y_contiguous&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 此时 y_contiguous 不再与 x_non_contiguous 共享存储</span></span><br></pre></td></tr></table></figure>

<h4 id="逐元素操作"><a href="#逐元素操作" class="headerlink" title="逐元素操作"></a>逐元素操作</h4><p>这些操作对张量的每个元素应用一些操作，并返回相同形状的新张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original tensor x: <span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x.pow(2): <span class="subst">&#123;x.<span class="built_in">pow</span>(<span class="number">2</span>)&#125;</span>&quot;</span>) <span class="comment"># 元素平方</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x.sqrt(): <span class="subst">&#123;x.sqrt()&#125;</span>&quot;</span>) <span class="comment"># 元素开方</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x.rsqrt(): <span class="subst">&#123;x.rsqrt()&#125;</span>&quot;</span>) <span class="comment"># 元素倒数开方 (1 / sqrt(x_i))</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x + x: <span class="subst">&#123;x + x&#125;</span>&quot;</span>) <span class="comment"># 逐元素加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x * 2: <span class="subst">&#123;x * <span class="number">2</span>&#125;</span>&quot;</span>) <span class="comment"># 逐元素乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x / 0.5: <span class="subst">&#123;x / <span class="number">0.5</span>&#125;</span>&quot;</span>) <span class="comment"># 逐元素除法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># triu() - 获取矩阵的上三角部分，在注意力机制中很有用</span></span><br><span class="line">x_triu = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original 3x3 tensor:\n<span class="subst">&#123;x_triu&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x_triu.triu():\n<span class="subst">&#123;x_triu.triu()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 这对于计算因果注意力掩码很有用，其中 M[i, j] 表示 i 对 j 的贡献。</span></span><br></pre></td></tr></table></figure>

<h4 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h4><p>矩阵乘法是深度学习的核心操作，尤其是在神经网络层中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">16</span>, <span class="number">32</span>) <span class="comment"># 输入张量</span></span><br><span class="line">w = torch.ones(<span class="number">32</span>, <span class="number">2</span>)  <span class="comment"># 权重张量</span></span><br><span class="line">y = x @ w              <span class="comment"># 矩阵乘法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of x: <span class="subst">&#123;x.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of w: <span class="subst">&#123;w.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of y (x @ w): <span class="subst">&#123;y.size()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批处理和序列 - 在深度学习中常见，通常有批次和序列维度</span></span><br><span class="line"><span class="comment"># 例如，一个批次中包含多个序列，每个序列中的每个token都有一个特征向量</span></span><br><span class="line">x_batch = torch.ones(<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>) <span class="comment"># (batch_size, sequence_length, num_heads, hidden_dim)</span></span><br><span class="line">w_weight = torch.ones(<span class="number">32</span>, <span class="number">2</span>)       <span class="comment"># (hidden_dim, output_dim)</span></span><br><span class="line">y_output = x_batch @ w_weight</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of x_batch: <span class="subst">&#123;x_batch.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of w_weight: <span class="subst">&#123;w_weight.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of y_output (x_batch @ w_weight): <span class="subst">&#123;y_output.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 在这种情况下，我们迭代 x 的前两个维度（批次和序列），并与 w 进行乘法。</span></span><br></pre></td></tr></table></figure>

<p><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/5a5ec796bf214a54b5dd6311b773b935.png" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/5a5ec796bf214a54b5dd6311b773b935.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="在这里插入图片描述"></p>
<h3 id="Einops"><a href="#Einops" class="headerlink" title="Einops"></a>Einops</h3><p>Einops 是一个用于操作命名维度张量的库，灵感来自爱因斯坦求和约定。它提供了一种更直观、更不易出错的方式来重塑、转置和聚合张量。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://einops.rocks/1-einops-basics/">Einops 教程</a></li>
</ul>
<h4 id="Jaxtyping-基础"><a href="#Jaxtyping-基础" class="headerlink" title="Jaxtyping 基础"></a>Jaxtyping 基础</h4><p>Jaxtyping 是一种类型提示工具，用于在代码中明确张量的维度名称，提高代码可读性和可维护性（仅作文档，不强制执行）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> jaxtyping <span class="keyword">import</span> Float</span><br><span class="line"></span><br><span class="line"><span class="comment"># 旧方式：维度含义不明确</span></span><br><span class="line">x_old = torch.ones(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># batch seq heads hidden</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Old way tensor shape: <span class="subst">&#123;x_old.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新方式 (jaxtyping): 明确命名维度</span></span><br><span class="line">x_new: Float[torch.Tensor, <span class="string">&quot;batch seq heads hidden&quot;</span>] = torch.ones(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New way (jaxtyping) tensor shape: <span class="subst">&#123;x_new.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 注意：这只是文档，没有运行时强制执行。</span></span><br></pre></td></tr></table></figure>

<h4 id="Einops-Einsum"><a href="#Einops-Einsum" class="headerlink" title="Einops Einsum"></a>Einops Einsum</h4><p><code>einsum</code> 是带有良好簿记的广义矩阵乘法。它允许通过指定输入和输出张量的维度名称来执行复杂的张量操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> einsum</span><br><span class="line"><span class="keyword">from</span> jaxtyping <span class="keyword">import</span> Float</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个张量，并使用 jaxtyping 命名维度</span></span><br><span class="line">x: Float[torch.Tensor, <span class="string">&quot;batch seq1 hidden&quot;</span>] = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">y: Float[torch.Tensor, <span class="string">&quot;batch seq2 hidden&quot;</span>] = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of x: <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of y: <span class="subst">&#123;y.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 旧方式：容易混淆维度</span></span><br><span class="line">z_old = x @ y.transpose(-<span class="number">2</span>, -<span class="number">1</span>) <span class="comment"># batch, sequence, sequence</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Old way (x @ y.transpose): <span class="subst">&#123;z_old.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新方式 (einops einsum): 明确指定维度操作</span></span><br><span class="line">z_new = einsum(x, y, <span class="string">&quot;batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New way (einops einsum): <span class="subst">&#123;z_new.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 在输出中未命名的维度将被求和。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用 `...` 来表示广播任意数量的维度</span></span><br><span class="line">z_broadcast = einsum(x, y, <span class="string">&quot;... seq1 hidden, ... seq2 hidden -&gt; ... seq1 seq2&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Einops einsum with broadcast (...): <span class="subst">&#123;z_broadcast.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Einops-Reduce"><a href="#Einops-Reduce" class="headerlink" title="Einops Reduce"></a>Einops Reduce</h4><p><code>reduce</code> 允许通过一些操作（例如 sum, mean, max, min）减少单个张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">from</span> jaxtyping <span class="keyword">import</span> Float</span><br><span class="line"></span><br><span class="line">x: Float[torch.Tensor, <span class="string">&quot;batch seq hidden&quot;</span>] = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original tensor x shape: <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 旧方式：使用 PyTorch 的 mean 函数</span></span><br><span class="line">y_old = x.mean(dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Old way (x.mean(dim=-1)): <span class="subst">&#123;y_old.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新方式 (einops reduce): 明确指定操作和维度</span></span><br><span class="line">y_new = reduce(x, <span class="string">&quot;... hidden -&gt; ...&quot;</span>, <span class="string">&quot;sum&quot;</span>) <span class="comment"># 对 hidden 维度求和</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New way (einops reduce sum): <span class="subst">&#123;y_new.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">y_mean = reduce(x, <span class="string">&quot;... hidden -&gt; ...&quot;</span>, <span class="string">&quot;mean&quot;</span>) <span class="comment"># 对 hidden 维度求平均</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;New way (einops reduce mean): <span class="subst">&#123;y_mean.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Einops-Rearrange"><a href="#Einops-Rearrange" class="headerlink" title="Einops Rearrange"></a>Einops Rearrange</h4><p><code>rearrange</code> 用于重塑张量，当一个维度实际上代表两个或多个逻辑维度时，可以将其拆分或组合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, einsum</span><br><span class="line"><span class="keyword">from</span> jaxtyping <span class="keyword">import</span> Float</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设有一个张量，其中 total_hidden 维度是 heads * hidden1 的扁平表示</span></span><br><span class="line">x: Float[torch.Tensor, <span class="string">&quot;batch seq total_hidden&quot;</span>] = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original tensor x shape: <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分 `total_hidden` 为两个维度 (`heads` 和 `hidden1`)</span></span><br><span class="line"><span class="comment"># 假设 heads = 2，则 hidden1 = total_hidden / heads = 8 / 2 = 4</span></span><br><span class="line">x_rearranged = rearrange(x, <span class="string">&quot;... (heads hidden1) -&gt; ... heads hidden1&quot;</span>, heads=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After rearrange (split total_hidden): <span class="subst">&#123;x_rearranged.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设有一个权重矩阵 w 用于转换 hidden1 到 hidden2</span></span><br><span class="line">w: Float[torch.Tensor, <span class="string">&quot;hidden1 hidden2&quot;</span>] = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Weight matrix w shape: <span class="subst">&#123;w.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行转换 (例如，多头注意力中的线性投影)</span></span><br><span class="line">x_transformed = einsum(x_rearranged, w, <span class="string">&quot;... hidden1, hidden1 hidden2 -&gt; ... hidden2&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After einsum (transform hidden1 to hidden2): <span class="subst">&#123;x_transformed.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 `heads` 和 `hidden2` 重新组合成一个维度</span></span><br><span class="line">x_combined = rearrange(x_transformed, <span class="string">&quot;... heads hidden2 -&gt; ... (heads hidden2)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After rearrange (combine heads and hidden2): <span class="subst">&#123;x_combined.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="张量操作-FLOPs"><a href="#张量操作-FLOPs" class="headerlink" title="张量操作 FLOPs"></a>张量操作 FLOPs</h3><p>浮点运算 (FLOP) 是基本操作，如加法或乘法。计算 FLOPs 对于评估模型计算成本至关重要。</p>
<h4 id="线性层-FLOPs"><a href="#线性层-FLOPs" class="headerlink" title="线性层 FLOPs"></a>线性层 FLOPs</h4><p>对于一个输入 <code>x</code> (B, D)，权重 <code>w</code> (D, K)，输出 <code>y</code> (B, K) 的线性层 <code>y = x @ w</code>，其 FLOPs 计算如下：</p>
<ul>
<li>矩阵乘法 <code>x @ w</code>：每个输出元素 <code>y_ik</code> 是 <code>sum_j (x_ij * w_jk)</code>。这涉及到 <code>D</code> 次乘法和 <code>D-1</code> 次加法。总共 <code>2D-1</code> 次浮点运算。对于 <code>B * K</code> 个输出元素，总 FLOPs 大约为 <code>B * K * (2D - 1)</code>，近似为 <code>2 * B * D * K</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 B=16, D=32, K=2</span></span><br><span class="line">B, D, K = <span class="number">16</span>, <span class="number">32</span>, <span class="number">2</span></span><br><span class="line">flops_linear_layer = <span class="number">2</span> * B * D * K</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;FLOPs for a linear layer (B=<span class="subst">&#123;B&#125;</span>, D=<span class="subst">&#123;D&#125;</span>, K=<span class="subst">&#123;K&#125;</span>): <span class="subst">&#123;flops_linear_layer&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="激活函数-FLOPs"><a href="#激活函数-FLOPs" class="headerlink" title="激活函数 FLOPs"></a>激活函数 FLOPs</h4><ul>
<li><code>ReLU(x)</code>: 对于每个元素，进行一次比较和一次可能的赋值，通常算作 <code>B * D</code> FLOPs。</li>
<li><code>Softmax(x)</code>: 涉及到指数运算和除法。对于 <code>B * D</code> 的张量，大约是 <code>2 * B * D</code> FLOPs。</li>
</ul>
<h4 id="损失函数-FLOPs"><a href="#损失函数-FLOPs" class="headerlink" title="损失函数 FLOPs"></a>损失函数 FLOPs</h4><ul>
<li><code>MSELoss(x, y)</code> (均方误差损失): <code>(x - y)^2</code> 涉及一次减法、一次乘法（平方），然后求和求平均。大约 <code>3 * B * D</code> FLOPs。</li>
</ul>
<h3 id="梯度基础"><a href="#梯度基础" class="headerlink" title="梯度基础"></a>梯度基础</h3><p>PyTorch 使用自动微分（反向传播）计算梯度。<code>loss.backward()</code> 会计算所有 <code>requires_grad=True</code> 的张量的梯度，并存储在 <code>param.grad</code> 属性中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的线性模型</span></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor([<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">1.5</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">y = x * w</span><br><span class="line">loss = y.<span class="built_in">sum</span>() <span class="comment"># 假设损失是 y 的和</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;x: <span class="subst">&#123;x&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;w: <span class="subst">&#123;w&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;y: <span class="subst">&#123;y&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of x: <span class="subst">&#123;x.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient of w: <span class="subst">&#123;w.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="梯度-FLOPs"><a href="#梯度-FLOPs" class="headerlink" title="梯度 FLOPs"></a>梯度 FLOPs</h3><p>计算梯度（反向传播）的 FLOPs 通常是前向传播的几倍。</p>
<ul>
<li>前向传播：<code>2 * (# 数据点) * (# 参数) FLOPs</code></li>
<li>反向传播：<code>4 * (# 数据点) * (# 参数) FLOPs</code></li>
<li>总计：<code>6 * (# 数据点) * (# 参数) FLOPs</code></li>
</ul>
<p>这个 <code>6 * (# 数据点) * (# 参数) FLOPs</code> 是一个常用的粗略估计，用于估算训练大型语言模型的计算量。</p>
<p>可视化：<br><img src="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/1a8cfeb573f3466496a7b61b0024213e.gif" class="lazyload" data-srcset="https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/1a8cfeb573f3466496a7b61b0024213e.gif" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="在这里插入图片描述"></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="模块参数"><a href="#模块参数" class="headerlink" title="模块参数"></a>模块参数</h3><p>模型参数存储为 <code>nn.Parameter</code> 对象，它们是特殊的张量，会被 PyTorch 自动跟踪梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_dim = <span class="number">16</span></span><br><span class="line">output_dim = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 nn.Parameter 对象</span></span><br><span class="line">w = nn.Parameter(torch.randn(input_dim, output_dim))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Type of w: <span class="subst">&#123;<span class="built_in">type</span>(w)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is w a Tensor? <span class="subst">&#123;<span class="built_in">isinstance</span>(w, torch.Tensor)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accessing underlying data: <span class="subst">&#123;<span class="built_in">type</span>(w.data)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h4><p>参数初始化对训练的稳定性和收敛速度至关重要。不当的初始化可能导致梯度爆炸或消失。</p>
<ul>
<li>初始值过大可能导致梯度爆炸，训练不稳定。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 演示不当初始化可能导致的问题</span></span><br><span class="line">x_input = torch.randn(input_dim)</span><br><span class="line">w_bad_init = nn.Parameter(torch.randn(input_dim, output_dim))</span><br><span class="line">output_bad = x_input @ w_bad_init</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output with bad initialization (first element): <span class="subst">&#123;output_bad[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 注意：这里 output_bad[0] 的值可能会很大，因为它没有进行缩放。</span></span><br><span class="line"><span class="comment"># 理论上，output 的每个元素会随着 input_dim 的平方根而缩放。</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Xavier 初始化：通过 <code>1/sqrt(input_dim)</code> 重新缩放，使输出值与 <code>input_dim</code> 无关，有助于保持激活值的方差稳定。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Xavier 初始化 (或 Glorot 初始化) 的思想</span></span><br><span class="line">w_xavier = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))</span><br><span class="line">output_xavier = x_input @ w_xavier</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output with Xavier initialization (first element): <span class="subst">&#123;output_xavier[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 此时 output_xavier[0] 的值会更稳定，不会随 input_dim 显著变化。</span></span><br></pre></td></tr></table></figure>

<ul>
<li><a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Xavier 初始化论文</a></li>
<li>为避免异常值，可以将正态分布截断到 [-3, 3]。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 截断正态分布初始化，进一步提高稳定性</span></span><br><span class="line">w_trunc_normal = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, output_dim), std=<span class="number">1</span> / np.sqrt(input_dim), a=-<span class="number">3</span>, b=<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Weight initialized with truncated normal:\n<span class="subst">&#123;w_trunc_normal&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="自定义模型"><a href="#自定义模型" class="headerlink" title="自定义模型"></a>自定义模型</h3><p>使用 <code>nn.Parameter</code> 和 <code>nn.Module</code> 构建简单的深度线性模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 辅助函数，用于获取设备 (CPU 或 GPU)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_device</span>():</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的线性层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim: <span class="built_in">int</span>, output_dim: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 使用 Xavier 风格初始化权重</span></span><br><span class="line">        self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> x @ self.weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度线性模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cruncher</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, num_layers: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 使用 ModuleList 存储多个线性层</span></span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            Linear(dim, dim)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.final = Linear(dim, <span class="number">1</span>) <span class="comment"># 最终输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># 应用线性层</span></span><br><span class="line">        B, D = x.size()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用最终输出层</span></span><br><span class="line">        x = self.final(x)</span><br><span class="line">        <span class="keyword">assert</span> x.size() == torch.Size([B, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 移除最后一个维度，使其变为 (B,)</span></span><br><span class="line">        x = x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">assert</span> x.size() == torch.Size([B])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型并检查参数数量</span></span><br><span class="line">D = <span class="number">64</span>  <span class="comment"># 维度</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">model = Cruncher(dim=D, num_layers=num_layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查模型参数</span></span><br><span class="line">param_sizes = [</span><br><span class="line">    (name, param.numel())</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.state_dict().items()</span><br><span class="line">]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model parameter sizes: <span class="subst">&#123;param_sizes&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算总参数数量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_num_parameters</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters())</span><br><span class="line"></span><br><span class="line">num_parameters = get_num_parameters(model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total number of parameters: <span class="subst">&#123;num_parameters&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型移动到 GPU</span></span><br><span class="line">device = get_device()</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model device: <span class="subst">&#123;<span class="built_in">next</span>(model.parameters()).device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在数据上运行模型</span></span><br><span class="line">B = <span class="number">8</span>  <span class="comment"># 批次大小</span></span><br><span class="line">x_data = torch.randn(B, D, device=device)</span><br><span class="line">y_output = model(x_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Input data shape: <span class="subst">&#123;x_data.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Output data shape: <span class="subst">&#123;y_output.size()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="训练循环和最佳实践"><a href="#训练循环和最佳实践" class="headerlink" title="训练循环和最佳实践"></a>训练循环和最佳实践</h2><h3 id="随机性注意事项"><a href="#随机性注意事项" class="headerlink" title="随机性注意事项"></a>随机性注意事项</h3><p>随机性出现在参数初始化、Dropout、数据排序等许多地方。为确保可复现性，建议为每次随机性使用不同的随机种子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Random seed set to <span class="subst">&#123;seed&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证随机性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Torch random number: <span class="subst">&#123;torch.randn(<span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;NumPy random number: <span class="subst">&#123;np.random.rand(<span class="number">1</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Python random number: <span class="subst">&#123;random.random()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><p>语言模型中的数据是整数序列（由分词器输出）。通常，数据量非常大，不能一次性加载到内存中。</p>
<ul>
<li>将数据序列化为 NumPy 数组。</li>
<li>使用 <code>np.memmap</code> 惰性加载数据，避免一次性加载全部数据到内存（例如 LLaMA 数据为 2.8TB）。<code>memmap</code> 允许像访问内存数组一样访问磁盘上的文件。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟原始数据并保存到文件</span></span><br><span class="line">orig_data = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>)], dtype=np.int32)</span><br><span class="line">orig_data.tofile(<span class="string">&quot;data.npy&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original data saved to data.npy: <span class="subst">&#123;orig_data&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 memmap 惰性加载数据</span></span><br><span class="line">data_memmap = np.memmap(<span class="string">&quot;data.npy&quot;</span>, dtype=np.int32, mode=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Data loaded with memmap (first 10 elements): <span class="subst">&#123;data_memmap[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Is data_memmap equal to original data? <span class="subst">&#123;np.array_equal(data_memmap, orig_data)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载器示例 (get_batch 函数)</span></span><br><span class="line"><span class="comment"># 这是一个简化的 get_batch 函数，用于从 memmap 数据中获取批次</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">data: np.array, batch_size: <span class="built_in">int</span>, sequence_length: <span class="built_in">int</span>, device: <span class="built_in">str</span></span>) -&gt; torch.Tensor:</span><br><span class="line">    start_indices = torch.randint(<span class="built_in">len</span>(data) - sequence_length, (batch_size,))</span><br><span class="line">    x = torch.tensor([data[start:start + sequence_length] <span class="keyword">for</span> start <span class="keyword">in</span> start_indices])</span><br><span class="line">    <span class="keyword">return</span> x.to(device)</span><br><span class="line"></span><br><span class="line">B = <span class="number">2</span>  <span class="comment"># 批次大小</span></span><br><span class="line">L = <span class="number">4</span>  <span class="comment"># 序列长度</span></span><br><span class="line">x_batch = get_batch(data_memmap, batch_size=B, sequence_length=L, device=get_device())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Example batch from data loader (shape: <span class="subst">&#123;x_batch.size()&#125;</span>):\n<span class="subst">&#123;x_batch&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="固定内存-Pinned-Memory"><a href="#固定内存-Pinned-Memory" class="headerlink" title="固定内存 (Pinned Memory)"></a>固定内存 (Pinned Memory)</h4><p>默认情况下，CPU 张量在分页内存中。通过 <code>pin_memory()</code> 将其固定在可分页内存中，可以允许从 CPU 异步复制到 GPU，从而提高数据传输效率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    <span class="comment"># 创建一个 CPU 张量</span></span><br><span class="line">    cpu_tensor = torch.randn(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;CPU tensor device: <span class="subst">&#123;cpu_tensor.device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 固定内存</span></span><br><span class="line">    pinned_tensor = cpu_tensor.pin_memory()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Pinned tensor device: <span class="subst">&#123;pinned_tensor.device&#125;</span>&quot;</span>) <span class="comment"># 仍然是 CPU，但已固定</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 异步复制到 GPU</span></span><br><span class="line">    gpu_tensor = pinned_tensor.to(<span class="string">&quot;cuda&quot;</span>, non_blocking=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GPU tensor device (non_blocking): <span class="subst">&#123;gpu_tensor.device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这允许在数据传输的同时进行 GPU 计算，实现并行化。</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA is not available. Pinned memory demonstration skipped.&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>优化器负责根据计算出的梯度更新模型参数。本节介绍了几种常见的优化器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 辅助函数，用于获取设备 (CPU 或 GPU)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_device</span>():</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的线性层 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim: <span class="built_in">int</span>, output_dim: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> x @ self.weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度线性模型 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cruncher</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, num_layers: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            Linear(dim, dim)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.final = Linear(dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        B, D = x.size()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        x = self.final(x)</span><br><span class="line">        x = x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD 优化器实现示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>(torch.optim.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: Iterable[nn.Parameter], lr: <span class="built_in">float</span> = <span class="number">0.01</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SGD, self).__init__(params, <span class="built_in">dict</span>(lr=lr))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            lr = group[<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 检查梯度是否存在</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                p.data -= lr * grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaGrad 优化器实现示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdaGrad</span>(torch.optim.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: Iterable[nn.Parameter], lr: <span class="built_in">float</span> = <span class="number">0.01</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AdaGrad, self).__init__(params, <span class="built_in">dict</span>(lr=lr))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            lr = group[<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment"># 检查梯度是否存在</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                state = self.state[p]</span><br><span class="line">                grad = p.grad.data</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 获取平方梯度 g2 = sum_&#123;i&lt;t&#125; g_i^2</span></span><br><span class="line">                g2 = state.get(<span class="string">&quot;g2&quot;</span>, torch.zeros_like(grad))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 更新优化器状态</span></span><br><span class="line">                g2 += torch.square(grad)</span><br><span class="line">                state[<span class="string">&quot;g2&quot;</span>] = g2</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 更新参数</span></span><br><span class="line">                p.data -= lr * grad / torch.sqrt(g2 + <span class="number">1e-5</span>) <span class="comment"># 1e-5 防止除以零</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 演示优化器使用</span></span><br><span class="line">B = <span class="number">2</span></span><br><span class="line">D = <span class="number">4</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">model = Cruncher(dim=D, num_layers=num_layers).to(get_device())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 AdaGrad 优化器</span></span><br><span class="line">optimizer = AdaGrad(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据和损失计算</span></span><br><span class="line">x_opt = torch.randn(B, D, device=get_device())</span><br><span class="line">y_target = torch.tensor([<span class="number">4.</span>, <span class="number">5.</span>], device=get_device())</span><br><span class="line">pred_y = model(x_opt)</span><br><span class="line">loss = F.mse_loss(<span class="built_in">input</span>=pred_y, target=y_target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Initial model parameters (first layer weight):\n<span class="subst">&#123;model.layers[<span class="number">0</span>].weight.data&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行一步优化</span></span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model parameters after one optimization step (first layer weight):\n<span class="subst">&#123;model.layers[<span class="number">0</span>].weight.data&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清零梯度 (重要步骤，防止梯度累积)</span></span><br><span class="line">optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Gradients cleared.&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h4><p>训练模型所需的总内存是参数、激活、梯度和优化器状态内存的总和。</p>
<ul>
<li>参数内存：模型中所有参数的内存。</li>
<li>激活内存：前向传播过程中各层输出的激活值的内存。</li>
<li>梯度内存：与参数对应的梯度内存。</li>
<li>优化器状态内存：优化器（如 AdamW）可能需要额外的状态（如动量、方差估计）来更新参数。</li>
</ul>
<p>假设 float32 (4 字节&#x2F;元素)：</p>
<p><code>总内存 = 4 * (参数数量 + 激活数量 + 梯度数量 + 优化器状态数量)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 D=4, num_layers=2, B=2</span></span><br><span class="line">D = <span class="number">4</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">B = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数数量 (Cruncher 模型示例)</span></span><br><span class="line">num_parameters = (D * D * num_layers) + D</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of parameters: <span class="subst">&#123;num_parameters&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活数量 (简化估计，取决于模型结构和批次大小)</span></span><br><span class="line"><span class="comment"># 假设每个线性层输出 B*D 的激活，共 num_layers 层</span></span><br><span class="line">num_activations = B * D * num_layers</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of activations (estimated): <span class="subst">&#123;num_activations&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度数量 (通常与参数数量相同)</span></span><br><span class="line">num_gradients = num_parameters</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of gradients: <span class="subst">&#123;num_gradients&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器状态数量 (对于 AdaGrad，每个参数有一个 g2 状态)</span></span><br><span class="line">num_optimizer_states = num_parameters</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of optimizer states (AdaGrad): <span class="subst">&#123;num_optimizer_states&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 总内存 (假设 float32)</span></span><br><span class="line">total_memory_bytes = <span class="number">4</span> * (num_parameters + num_activations + num_gradients + num_optimizer_states)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total estimated memory (bytes, float32): <span class="subst">&#123;total_memory_bytes&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total estimated memory (MB, float32): <span class="subst">&#123;total_memory_bytes / (<span class="number">1024</span>**<span class="number">2</span>):<span class="number">.2</span>f&#125;</span> MB&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="计算-一步"><a href="#计算-一步" class="headerlink" title="计算 (一步)"></a>计算 (一步)</h4><p>对于一步训练（前向传播 + 反向传播），总 FLOPs 大致为：</p>
<p><code>FLOPs = 6 * B * num_parameters</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 B=2, num_parameters=36 (从上面计算得到)</span></span><br><span class="line">flops_one_step = <span class="number">6</span> * B * num_parameters</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Estimated FLOPs for one training step: <span class="subst">&#123;flops_one_step&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h3><p>一个典型的训练循环包括数据获取、前向传播、损失计算、反向传播、参数更新和梯度清零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 辅助函数，用于获取设备 (CPU 或 GPU)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_device</span>():</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的线性层 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim: <span class="built_in">int</span>, output_dim: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> x @ self.weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度线性模型 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cruncher</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, num_layers: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            Linear(dim, dim)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.final = Linear(dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        B, D = x.size()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        x = self.final(x)</span><br><span class="line">        x = x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD 优化器实现示例 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>(torch.optim.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: Iterable[nn.Parameter], lr: <span class="built_in">float</span> = <span class="number">0.01</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SGD, self).__init__(params, <span class="built_in">dict</span>(lr=lr))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            lr = group[<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                p.data -= lr * grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据生成函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch_train</span>(<span class="params">B: <span class="built_in">int</span>, D: <span class="built_in">int</span>, true_w: torch.Tensor</span>) -&gt; <span class="built_in">tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line">    x = torch.randn(B, D).to(get_device())</span><br><span class="line">    true_y = x @ true_w</span><br><span class="line">    <span class="keyword">return</span> (x, true_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params"></span></span><br><span class="line"><span class="params">    name: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    get_batch_fn,</span></span><br><span class="line"><span class="params">    D: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    num_layers: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    B: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    num_train_steps: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    lr: <span class="built_in">float</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n--- Starting training for: <span class="subst">&#123;name&#125;</span> ---&quot;</span>)</span><br><span class="line">    model = Cruncher(dim=D, num_layers=num_layers).to(get_device())</span><br><span class="line">    optimizer = SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟真实权重，用于生成数据</span></span><br><span class="line">    true_w = torch.arange(D, dtype=torch.float32, device=get_device())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_train_steps):</span><br><span class="line">        <span class="comment"># 1. 获取数据</span></span><br><span class="line">        x, y = get_batch_fn(B=B, D=D, true_w=true_w)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 前向传播 (计算损失)</span></span><br><span class="line">        pred_y = model(x)</span><br><span class="line">        loss = F.mse_loss(pred_y, y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 反向传播 (计算梯度)</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 清零梯度</span></span><br><span class="line">        optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (t + <span class="number">1</span>) % (num_train_steps // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> t == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Step <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_train_steps&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;--- Training for <span class="subst">&#123;name&#125;</span> finished ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行一个简单的训练运行</span></span><br><span class="line">train(<span class="string">&quot;simple_run&quot;</span>, get_batch_train, D=<span class="number">16</span>, num_layers=<span class="number">0</span>, B=<span class="number">4</span>, num_train_steps=<span class="number">10</span>, lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 演示超参数调整的影响 (例如，更高的学习率)</span></span><br><span class="line">train(<span class="string">&quot;hyperparameter_tuning_lr_0.1&quot;</span>, get_batch_train, D=<span class="number">16</span>, num_layers=<span class="number">0</span>, B=<span class="number">4</span>, num_train_steps=<span class="number">10</span>, lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="检查点"><a href="#检查点" class="headerlink" title="检查点"></a>检查点</h3><p>训练大型语言模型需要很长时间，并且可能会因为各种原因中断。为了避免丢失所有进度，建议定期将模型和优化器状态保存到磁盘（创建检查点）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 辅助函数，用于获取设备 (CPU 或 GPU)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_device</span>():</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的线性层 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim: <span class="built_in">int</span>, output_dim: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> x @ self.weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度线性模型 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cruncher</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, num_layers: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            Linear(dim, dim)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.final = Linear(dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        B, D = x.size()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        x = self.final(x)</span><br><span class="line">        x = x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaGrad 优化器实现示例 (同上)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdaGrad</span>(torch.optim.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: Iterable[nn.Parameter], lr: <span class="built_in">float</span> = <span class="number">0.01</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AdaGrad, self).__init__(params, <span class="built_in">dict</span>(lr=lr))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            lr = group[<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                state = self.state[p]</span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                g2 = state.get(<span class="string">&quot;g2&quot;</span>, torch.zeros_like(grad))</span><br><span class="line">                g2 += torch.square(grad)</span><br><span class="line">                state[<span class="string">&quot;g2&quot;</span>] = g2</span><br><span class="line">                p.data -= lr * grad / torch.sqrt(g2 + <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型和优化器</span></span><br><span class="line">model = Cruncher(dim=<span class="number">64</span>, num_layers=<span class="number">3</span>).to(get_device())</span><br><span class="line">optimizer = AdaGrad(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设模型已经训练了一段时间，这里我们只是模拟</span></span><br><span class="line"><span class="comment"># model.load_state_dict(...) 等</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存检查点</span></span><br><span class="line">checkpoint = &#123;</span><br><span class="line">    <span class="string">&quot;model_state_dict&quot;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),</span><br><span class="line">    <span class="string">&quot;epoch&quot;</span>: <span class="number">10</span>, <span class="comment"># 可以保存其他训练信息，如当前 epoch</span></span><br><span class="line">    <span class="string">&quot;loss&quot;</span>: <span class="number">0.05</span></span><br><span class="line">&#125;</span><br><span class="line">torch.save(checkpoint, <span class="string">&quot;model_checkpoint.pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Checkpoint saved to model_checkpoint.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载检查点</span></span><br><span class="line">loaded_checkpoint = torch.load(<span class="string">&quot;model_checkpoint.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型和优化器状态</span></span><br><span class="line">loaded_model = Cruncher(dim=<span class="number">64</span>, num_layers=<span class="number">3</span>).to(get_device())</span><br><span class="line">loaded_optimizer = AdaGrad(loaded_model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">loaded_model.load_state_dict(loaded_checkpoint[<span class="string">&quot;model_state_dict&quot;</span>])</span><br><span class="line">loaded_optimizer.load_state_dict(loaded_checkpoint[<span class="string">&quot;optimizer_state_dict&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Checkpoint loaded successfully.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loaded epoch: <span class="subst">&#123;loaded_checkpoint[<span class="string">&#x27;epoch&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loaded loss: <span class="subst">&#123;loaded_checkpoint[<span class="string">&#x27;loss&#x27;</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h3><p>数据类型（float32、bfloat16、fp8）各有优缺点。为了兼顾精度、内存和计算效率，可以采用混合精度训练。</p>
<ul>
<li><strong>高精度</strong>: 更准确&#x2F;稳定，更多内存，更多计算。</li>
<li><strong>低精度</strong>: 精度&#x2F;稳定性较低，更少内存，更少计算。</li>
</ul>
<p><strong>如何兼顾？</strong></p>
<p>解决方案：默认使用 float32，但在可能的情况下使用 bfloat16 或 fp8。</p>
<ul>
<li><p><strong>具体方案</strong>：</p>
<ul>
<li>前向传播（激活）使用 bfloat16 或 fp8。</li>
<li>其余（参数、梯度）使用 float32。</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.03740.pdf">混合精度训练论文</a></p>
</li>
<li><p>PyTorch 提供了自动混合精度 (AMP) 库，可以简化混合精度训练的实现。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/amp.html">PyTorch AMP 文档</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/">NVIDIA 混合精度训练指南</a></li>
</ul>
</li>
<li><p>NVIDIA 的 Transformer Engine 支持线性层的 FP8，并提供了在整个训练过程中广泛使用 FP8 的方法。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.18313.pdf">使用 FP8 普遍训练</a></li>
</ul>
</li>
</ul>
<p>通过混合精度训练，可以在保持模型性能的同时，显著减少内存占用和提高训练速度。</p>

  </div>
  
  
    
    <div class='footer'>
       <!-- 参考资料、相关资料等 -->
      
       <!-- 相关文章 -->
      
      <!-- 版权声明组件 -->
      
        
          <div class='copyright'>
            <blockquote>
              
                
                  <p>博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</p>

                
              
                
                  <p>本文永久链接是：<a href="https://mengfanjun020906.github.io/2025/08/30/Standford CS336（二）训练模型介绍/">https://mengfanjun020906.github.io/2025/08/30/Standford CS336（二）训练模型介绍/</a></p>
                
              
            </blockquote>
          </div>
        
      
      <!-- 打赏组件 -->
      
    </div>
  
  
    


  <div class='article-meta' id="bottom">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateModified" datetime="2025-12-12T22:25:01+08:00">
  <a class='notlink'>
    <i class="fa-solid fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：Dec 12, 2025</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/%E5%AD%A6%E4%B9%A0/" rel="nofollow"><i class="fa-solid fa-hashtag fa-fw" aria-hidden="true"></i><p>学习</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" rel="nofollow"><i class="fa-solid fa-hashtag fa-fw" aria-hidden="true"></i><p>大模型</p></a></div>
  <span hidden itemprop="keywords">学习 大模型</span>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://mengfanjun020906.github.io/2025/08/30/Standford%20CS336%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/&title=Standford CS336（二）训练模型介绍 - MengFanjun的博客&summary="
          
          >
          
            <img src="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/128/qq.png" class="lazyload" data-srcset="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/128/qq.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://mengfanjun020906.github.io/2025/08/30/Standford%20CS336%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/&title=Standford CS336（二）训练模型介绍 - MengFanjun的博客&summary="
          
          >
          
            <img src="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/128/qzone.png" class="lazyload" data-srcset="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/128/qzone.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://service.weibo.com/share/share.php?url=https://mengfanjun020906.github.io/2025/08/30/Standford%20CS336%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/&title=Standford CS336（二）训练模型介绍 - MengFanjun的博客&summary="
          
          >
          
            <img src="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/128/weibo.png" class="lazyload" data-srcset="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/128/weibo.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">
          
        </a>
      
    
      
    
      
    
  </div>
</div>



        
      
    </div>
    <!-- Custom Files bottomMeta begin -->
    
    <!-- Custom Files bottomMeta end -->
  </div>


  
  

  
    <div class="prev-next">
      
        <a class='prev' href='/2025/10/02/Stanford%20CS336%20assignment1%EF%BC%88%E4%B8%8A%EF%BC%89/'>
          <p class='title'><i class="fa-solid fa-chevron-left" aria-hidden="true"></i>Stanford CS336 assignment1（上）</p>
          <p class='content'>源仓库链接：https://github.com/stanford-cs336/assignment1-basics
介绍
Byte-Pair Encoding (BPE) TokenizerT...</p>
        </a>
      
      
        <a class='next' href='/2025/08/21/Standford%20CS336%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D/'>
          <p class='title'>Standford CS336（一）课程介绍<i class="fa-solid fa-chevron-right" aria-hidden="true"></i></p>
          <p class='content'>仓库链接：https://github.com/stanford-cs336/spring2025-lectures
斯坦福CS336：从零开始构建语言模型 (2025春季)课程简介本课程是斯坦...</p>
        </a>
      
    </div>
  
  <!-- Custom Files postEnd begin-->
  
  <!-- Custom Files postEnd end-->
</article>


  


  <article class="post white-box shadow floatable blur" id="comments">
    <span hidden>
      <meta itemprop="discussionUrl" content="/2025/08/30/Standford%20CS336%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/index.html#comments">
    </span>
    <p ct><i class='fa-solid fa-comments'></i> 评论</p>
    

    <div id="layoutHelper-comments"></div>

  </article>






</div>
<aside id='l_side' itemscope itemtype="http://schema.org/WPSideBar">
  

  
    
    
      
    
  


<div class="widget-sticky pjax">

  
  


  <section class="widget toc-wrapper desktop mobile " id="toc-div" >
    
  <header>
    
      <i class="fa-solid fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B5%84%E6%BA%90%E7%B1%BB%E5%9E%8B"><span class="toc-text">资源类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E6%A0%B8%E7%AE%97"><span class="toc-text">内存核算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E7%A1%80"><span class="toc-text">张量基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%86%85%E5%AD%98"><span class="toc-text">张量内存</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#float32-%E5%8D%95%E7%B2%BE%E5%BA%A6"><span class="toc-text">float32 (单精度)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#float16-%E5%8D%8A%E7%B2%BE%E5%BA%A6"><span class="toc-text">float16 (半精度)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bfloat16"><span class="toc-text">bfloat16</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#fp8"><span class="toc-text">fp8</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BD%B1%E5%93%8D"><span class="toc-text">训练影响</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%A0%B8%E7%AE%97"><span class="toc-text">计算核算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GPU-%E4%B8%8A%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-text">GPU 上的张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C"><span class="toc-text">张量操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%AD%98%E5%82%A8"><span class="toc-text">张量存储</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%88%87%E7%89%87"><span class="toc-text">张量切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%90%E5%85%83%E7%B4%A0%E6%93%8D%E4%BD%9C"><span class="toc-text">逐元素操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-text">矩阵乘法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Einops"><span class="toc-text">Einops</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Jaxtyping-%E5%9F%BA%E7%A1%80"><span class="toc-text">Jaxtyping 基础</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Einops-Einsum"><span class="toc-text">Einops Einsum</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Einops-Reduce"><span class="toc-text">Einops Reduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Einops-Rearrange"><span class="toc-text">Einops Rearrange</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C-FLOPs"><span class="toc-text">张量操作 FLOPs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%B1%82-FLOPs"><span class="toc-text">线性层 FLOPs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-FLOPs"><span class="toc-text">激活函数 FLOPs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-FLOPs"><span class="toc-text">损失函数 FLOPs</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%9F%BA%E7%A1%80"><span class="toc-text">梯度基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6-FLOPs"><span class="toc-text">梯度 FLOPs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E5%8F%82%E6%95%B0"><span class="toc-text">模块参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">参数初始化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-text">自定义模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%E5%92%8C%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-text">训练循环和最佳实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%80%A7%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-text">随机性注意事项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-text">数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A%E5%86%85%E5%AD%98-Pinned-Memory"><span class="toc-text">固定内存 (Pinned Memory)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98"><span class="toc-text">内存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97-%E4%B8%80%E6%AD%A5"><span class="toc-text">计算 (一步)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-text">训练循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="toc-text">检查点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-text">混合精度训练</span></a></li></ol></li></ol>
    </div>
  </section>

  

</div>


<!-- 没有 pjax 占位会报错 万恶的 pjax -->

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <div class="pjax">
    <!-- pjax占位 -->
  </div>

  <!-- Custom Files side begin -->
  
  <!-- Custom Files side end -->
</aside>



          <!--此文件用来存放一些不方便取值的变量-->
<!--思路大概是将值藏到重加载的区域内-->

<pjax>
<script>
  window.pdata={}
  pdata.ispage=true;
  pdata.commentPath="";
  pdata.commentPlaceholder="";
  pdata.commentConfig={};
  //  see: /layout/_partial/scripts/_ctrl/coverCtrl.ejs
  
    // header
    var l_header=document.getElementById("l_header");
    
    l_header.classList.remove("show");
    
    
      // cover
      var cover_wrapper=document.querySelector('#l_cover .cover-wrapper');
      var scroll_down=document.getElementById('scroll-down');
      cover_wrapper.id="half";
      cover_wrapper.style.display="";
      scroll_down.style.display="none";
    
  
</script>
</pjax>
        </div>
        
  
  <footer class="footer clearfix"  itemscope itemtype="http://schema.org/WPFooter">
    <br><br>
    
      
        <div class="aplayer-container">
          

  
    <meting-js
      theme='#1BCDFC'
      autoplay='true'
      volume='0.7'
      loop='all'
      order='list'
      fixed='true'
      list-max-height='320px'
      server='netease'
      type='playlist'
      id='993524571'
      list-folded='true'>
    </meting-js>
  


        </div>
      
    
      
        <br>
        <div class="social-wrapper" itemprop="about" itemscope itemtype="http://schema.org/Thing">
          
            
              <a href="mailto:mengfanjun_020906@outlook.com"
                class="social fas fa-envelope flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer" itemprop="url">
                
              </a>
            
          
            
              <a href="https://github.com/MengFanjun020906"
                class="social fab fa-github flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer" itemprop="url">
                
              </a>
            
          
        </div>
      
    
      
        <div><p>Blog content follows the <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License</a></p>
</div>
      
    
      
        Use
        <a href="https://github.com/volantis-x/hexo-theme-volantis/#5.7.7" target="_blank" class="codename">Volantis</a>
        as theme, total visits
          <span id="busuanzi_value_site_pv"><i class="fa-solid fa-loader fa-spin fa-fw" aria-hidden="true"></i></span>
          times
        
      
    
      
        <div class='copyright'>
        <p><a href="/">Copyright © 2022-2023 孟凡钧</a></p>

        </div>
      
    
    <!-- Custom Files footer begin-->
    
    <!-- Custom Files footer end-->
  </footer>


        <a id="s-top" class="fa-solid fa-arrow-up fa-fw" href="/" onclick="return false;" title="top"></a>
      </div>
    </div>
    <div>
      <script>
  /******************** volantis.dom ********************************/
  // 页面选择器 将dom对象缓存起来 see: /source/js/app.js etc.
  volantis.dom.bodyAnchor = volantis.dom.$(document.getElementById("safearea")); // 页面主体
  volantis.dom.topBtn = volantis.dom.$(document.getElementById('s-top')); // 向上
  volantis.dom.wrapper = volantis.dom.$(document.getElementById('wrapper')); // 整个导航栏
  volantis.dom.coverAnchor = volantis.dom.$(document.querySelector('#l_cover .cover-wrapper')); // 1个
  volantis.dom.switcher = volantis.dom.$(document.querySelector('#l_header .switcher .s-search')); // 搜索按钮   移动端 1个
  volantis.dom.header = volantis.dom.$(document.getElementById('l_header')); // 移动端导航栏
  volantis.dom.search = volantis.dom.$(document.querySelector('#l_header .m_search')); // 搜索框 桌面端 移动端 1个
  volantis.dom.mPhoneList = volantis.dom.$(document.querySelectorAll('#l_header .m-phone .list-v')); //  手机端 子菜单 多个
</script>

<script>
  
  volantis.css("https://unpkg.com/volantis-static@0.0.1654736714924/libs/@fortawesome/fontawesome-free/css/all.min.css");
  
  
  
</script>
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=bPcCRxC4bZtkgRsg7UgdnDBMTDZWpbJMNpbRzeeUhkQ">
</script>
<!-- required -->


<!-- internal -->

<script src="/js/app.js"></script>






<!-- rightmenu要在darkmode之前（ToggleButton） darkmode要在comments之前（volantis.dark.push）-->



<script>
  function loadIssuesJS() {
    
      const sites_api = document.getElementById('sites-api');
      if (sites_api != undefined && typeof SitesJS === 'undefined') {
        volantis.js("/js/plugins/tags/sites.js")
      }
    
    
      const friends_api = document.getElementById('friends-api');
      if (friends_api != undefined && typeof FriendsJS === 'undefined') {
        volantis.js("/js/plugins/tags/friends.js")
      }
    
    
      const contributors_api = document.getElementById('contributors-api');
      if (contributors_api != undefined && typeof ContributorsJS === 'undefined') {
        volantis.js("/js/plugins/tags/contributors.js")
      }
    
  };
  loadIssuesJS()
  volantis.pjax.push(()=>{
    loadIssuesJS();
  })

</script>




  <script defer src="https://unpkg.com/volantis-static@0.0.1654736714924/libs/vanilla-lazyload/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 0
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    lazyLoadInstance.update();
  });
  document.addEventListener('pjax:complete', function () {
    lazyLoadInstance.update();
  });
</script>




  

<script>
  window.FPConfig = {
	delay: 0,
	ignoreKeywords: ["#"],
	maxRPS: 6,
	hoverDelay: 0
  };
</script>
<script defer src="https://unpkg.com/volantis-static@0.0.1654736714924/libs/flying-pages/flying-pages.min.js"></script>







  <script>
  volantis.css("https://unpkg.com/volantis-static@0.0.1654736714924/libs/aplayer/dist/APlayer.min.css");
  (async () => {
    // APlayer 需要在  MetingJS 之前加载
    await volantis.js("https://unpkg.com/volantis-static@0.0.1654736714924/libs/aplayer/dist/APlayer.min.js")
    await volantis.js("https://unpkg.com/volantis-static@0.0.1654736714924/libs/meting/dist/Meting.min.js")
  
  })();

  function SetAPlayerPlugin(){
    let Metings = document.querySelectorAll('meting-js');
    if (Metings.length === 0) {return;};
    if (Metings[0].aplayer && Metings[0].aplayer.on) {
      // improve the accessibility https://web.dev/button-name/
      document.querySelectorAll(".aplayer-icon-menu").forEach(e=>{
        e.setAttribute("aria-label","Aplayer Menu")
      })
      // message see: /layout/_plugins/message/script.ejs
      
        try {
          setTimeout(() => {
            Metings.forEach((item, index) => {
              const aplayerItem = item.aplayer; if(!aplayerItem) return;
              const rightAplayerCheck = 'true' === 'true'
                && item.meta.id === '993524571';
              if(rightAplayerCheck && typeof RightMenuAplayer !="undefined") RightMenuAplayer.checkAPlayer();
              if(aplayerItem.events.events.play.every(item => {return item.name !== 'messagePlay'})) {
                aplayerItem.on('play', function messagePlay() {
                  let index = aplayerItem.list.index;
                  let title = aplayerItem.list.audios[index].title;
                  let artist = aplayerItem.list.audios[index].artist;
                  setTimeout(() => {
                    VolantisApp.message('音乐通知', title + ' - ' + artist, {
                      icon: 'fa-solid fa-play',
                      transitionIn: 'flipInX',
                      transitionOut: 'flipOutX'
                    });
                  }, 100)
                });
              }
              if(aplayerItem.events.events.pause.every(item => {return item.name !== 'messagePause'})) {
                aplayerItem.on('pause', function messagePause() {
                  let index = aplayerItem.list.index;
                  let title = aplayerItem.list.audios[index].title;
                  let artist = aplayerItem.list.audios[index].artist;
                  setTimeout(() => {
                    // 歌曲播放结束也会触发 pause 事件，为了避免错误提示，等待一会儿
                    if(aplayerItem.paused) {
                      VolantisApp.message('音乐通知', title + ' - ' + artist, {
                        icon: 'fa-solid fa-pause',
                        transitionIn: 'flipInX',
                        transitionOut: 'flipOutX'
                      });
                    }
                  }, 100)
                });
              }
            });
          }, 500)
        } catch (error) { console.error(error); }
      
    }else{
      volantis.requestAnimationFrame(SetAPlayerPlugin)
    }
  }

  document.addEventListener("DOMContentLoaded", ()=>{
    SetAPlayerPlugin();
  });
  volantis.pjax.push(SetAPlayerPlugin);
</script>




      <script>
  volantis.layoutHelper("comments",`<div id="giscus_container"></div>`)

  volantis.giscus = {};

  function check_giscus() {
    if (volantis.dark.mode === "dark") {
      volantis.giscus.Theme = 'dark';
    } else {
      volantis.giscus.Theme = 'light';
    }

    return document.getElementById("giscus_container");
  }

  function pjax_giscus() {
    const HEAD = check_giscus();
    if (!HEAD) return;
    let cfg = Object.assign({"theme":{"light":"light","dark":"dark"},"repo":"MengFanjun020906/comments","repo-id":"R_kgDOIjrvJg","category":"Announcements","category-id":"DIC_kwDOIjrvJs4CS6YN","mapping":"pathname","reactions-enabled":"1","emit-metadata":"0","lang":"zh-CN"},pdata.commentConfig)
    const script = document.createElement('script');
    script.setAttribute('src', 'https://giscus.app/client.js');
    Object.keys(cfg).forEach(k=>{
      if (k != "theme") {
        script.setAttribute('data-'+k, cfg[k]);
      }
    })
    script.setAttribute('data-theme', volantis.giscus.Theme);
    script.setAttribute('crossorigin', "anonymous");
    HEAD.appendChild(script);
  }

  function dark_giscus() {
    const HEAD = check_giscus();
    if (!HEAD) return;

    const message = {
      setConfig: {
        theme: volantis.giscus.Theme
      }
    };
    const giscusIframe = document.querySelector('iframe.giscus-frame');
    giscusIframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
  }
  pjax_giscus();
  volantis.pjax.push(pjax_giscus);
  volantis.dark.push(dark_giscus);
</script>

    




  <script defer src="https://gcore.jsdelivr.net/gh/volantis-x/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js" data-pjax></script>


<!-- optional -->

  <script>
  const SearchServiceDataPathRoot = ("/" || "/").endsWith("/") ?
    "/" || "/" :
    "//" || "/";
  const SearchServiceDataPath = SearchServiceDataPathRoot + "content.json";

  function loadSearchScript() {
    // see: layout/_partial/scripts/_ctrl/cdnCtrl.ejs
    return volantis.js("/js/search/hexo.js");
  }

  function loadSearchService() {
    loadSearchScript();
    document.querySelectorAll(".input.u-search-input").forEach((e) => {
      e.removeEventListener("focus", loadSearchService, false);
    });

    document.querySelectorAll(".u-search-form").forEach((e) => {
      e.addEventListener("submit", (event) => {
        event.preventDefault();
      }, false);
    });
  }

  // 打开并搜索 字符串 s
  function OpenSearch(s) {
    if (typeof SearchService === 'undefined')
      loadSearchScript().then(() => {
        SearchService.setQueryText(s);
        SearchService.search();
      });
    else {
      SearchService.setQueryText(s);
      SearchService.search();
    }
  }

  // 访问含有 ?s=xxx  的链接时打开搜索 // 与搜索引擎 structured data 相关: /scripts/helpers/structured-data/lib/config.js
  if (window.location.search && /^\?s=/g.test(window.location.search)) {
    let queryText = decodeURI(window.location.search)
      .replace(/\ /g, "-")
      .replace(/^\?s=/g, "");
    OpenSearch(queryText);
  }

  // 搜索输入框获取焦点时加载搜索
  document.querySelectorAll(".input.u-search-input").forEach((e) => {
    e.addEventListener("focus", loadSearchService, false);
  });
</script>







  <script>



  function pjax_highlightjs_copyCode(){
    if (!(document.querySelector(".highlight .code pre") ||
      document.querySelector(".article pre code"))) {
      return;
    }
    VolantisApp.utilCopyCode(".highlight .code pre, .article pre code")
  }
  volantis.requestAnimationFrame(pjax_highlightjs_copyCode)
  volantis.pjax.push(pjax_highlightjs_copyCode)

</script>










  <script>
  let imgs = ["https://mfjblog-pic.oss-cn-beijing.aliyuncs.com/img/1071826.jpg"];
  let index = 0;
  let IntervalParallax = null;

  function parallax(){
    let ParallaxWindow = document.querySelector("#parallax-window");
    
      ParallaxWindow = document.querySelector("html");
    
    Parallax.window = ParallaxWindow;
    Parallax.options.fade = 1500;
    Parallax.cache = 1;
    next_parallax();
    Parallax.init();
    if (imgs.length>1) {
      IntervalParallax = setInterval(function () {
        next_parallax();
      }, '10000');
    }
  }

  function next_parallax() {
    if (typeof Parallax == "undefined") {
      return
    }
    
    if (imgs.length>=1) {
      Parallax.options.src = imgs[index % imgs.length];
      Parallax.start();
      index++;
      if (Parallax.cache) {
        fetch(imgs[index % imgs.length] +"?t=" + new Date().getTime());
        if (index == imgs.length) {
          Parallax.cache = 0;
        }
      }
    }
  }
  var runningOnBrowser = typeof window !== "undefined";
  var isBot = runningOnBrowser && !("onscroll" in window) || typeof navigator !== "undefined" && /(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent);
  if (!isBot) {
    volantis.js('/js/plugins/parallax.js').then(()=>{
      parallax()
    })
    volantis.pjax.send(()=>{
      clearInterval(IntervalParallax)
    },"clearIntervalParallax");
    volantis.pjax.push(parallax);
  }
</script>




  <script>
  function load_swiper() {
    if (!document.querySelectorAll(".swiper-container")[0]) return;
    volantis.css("https://unpkg.com/volantis-static@0.0.1654736714924/libs/swiper/swiper-bundle.min.css");
    volantis.js("https://unpkg.com/volantis-static@0.0.1654736714924/libs/swiper/swiper-bundle.min.js").then(() => {
      pjax_swiper();
    });
  }

  load_swiper();

  function pjax_swiper() {
    volantis.swiper = new Swiper('.swiper-container', {
      slidesPerView: 'auto',
      spaceBetween: 8,
      centeredSlides: true,
      loop: true,
      pagination: {
        el: '.swiper-pagination',
        clickable: true,
      },
      navigation: {
        nextEl: '.swiper-button-next',
        prevEl: '.swiper-button-prev',
      },
    });
  }

  volantis.pjax.push(() => {
    if (!document.querySelectorAll(".swiper-container")[0]) return;
    if (typeof volantis.swiper === "undefined") {
      load_swiper();
    } else {
      pjax_swiper();
    }
  });
</script>


<!-- pjax 标签必须存在于所有页面 否则 pjax error -->
<pjax>

        <script>
  // https://github.com/theme-next/hexo-theme-next/blob/master/layout/_third-party/math/mathjax.swig
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = "https://unpkg.com/volantis-static@0.0.1654736714924/libs/mathjax/es5/tex-mml-chtml.js";
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    // 文章章节标题不能为 “MathJax” ，否则会报错。
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typesetPromise();
  }
</script>



</pjax>

<script>
  function listennSidebarTOC() {
    const navItems = document.querySelectorAll(".toc li");
    if (!navItems.length) return;
    let targets = []
    const sections = [...navItems].map((element) => {
      const link = element.querySelector(".toc-link");
      const target = document.getElementById(
        decodeURI(link.getAttribute("href")).replace("#", "")
      );
      targets.push(target)
      // 解除 a 标签 href 的 锚点定位, a 标签 href 的 锚点定位 会随机启用?? 产生错位???
      link.setAttribute("onclick","return false;")
      link.setAttribute("toc-action","toc-"+decodeURI(link.getAttribute("href")).replace("#", ""))
      link.setAttribute("href","/")
      // 配置 点击 触发新的锚点定位
      link.addEventListener("click", (event) => {
        event.preventDefault();
        // 这里的 addTop 是通过错位使得 toc 自动展开.
        volantis.scroll.to(target,{addTop: 5, observer:true})
        // Anchor id
        history.pushState(null, document.title, "#" + target.id);
      });
      return target;
    });

    function activateNavByIndex(target) {
      if (target.classList.contains("active-current")) return;

      document.querySelectorAll(".toc .active").forEach((element) => {
        element.classList.remove("active", "active-current");
      });
      target.classList.add("active", "active-current");
      let parent = target.parentNode;
      while (!parent.matches(".toc")) {
        if (parent.matches("li")) parent.classList.add("active");
        parent = parent.parentNode;
      }
    }

    // 方案一：
    volantis.activateNavIndex=0
    activateNavByIndex(navItems[volantis.activateNavIndex])
    volantis.scroll.push(()=>{
      if (targets[0].getBoundingClientRect().top >= 0) {
        volantis.activateNavIndex = 0
      }else if (targets[targets.length-1].getBoundingClientRect().top < 0) {
        volantis.activateNavIndex = targets.length-1
      } else {
        for (let index = 0; index < targets.length; index++) {
          const target0 = targets[index];
          const target1 = targets[(index+1)%targets.length];
          if (target0.getBoundingClientRect().top < 0&&target1.getBoundingClientRect().top >= 0) {
            volantis.activateNavIndex=index
            break;
          }
        }
      }
      activateNavByIndex(navItems[volantis.activateNavIndex])
    })

    // 方案二：
    // IntersectionObserver 不是完美精确到像素级别 也不是低延时性的
    // function findIndex(entries) {
    //   let index = 0;
    //   let entry = entries[index];
    //   if (entry.boundingClientRect.top > 0) {
    //     index = sections.indexOf(entry.target);
    //     return index === 0 ? 0 : index - 1;
    //   }
    //   for (; index < entries.length; index++) {
    //     if (entries[index].boundingClientRect.top <= 0) {
    //       entry = entries[index];
    //     } else {
    //       return sections.indexOf(entry.target);
    //     }
    //   }
    //   return sections.indexOf(entry.target);
    // }
    // function createIntersectionObserver(marginTop) {
    //   marginTop = Math.floor(marginTop + 10000);
    //   let intersectionObserver = new IntersectionObserver(
    //     (entries, observe) => {
    //       let scrollHeight = document.documentElement.scrollHeight;
    //       if (scrollHeight > marginTop) {
    //         observe.disconnect();
    //         createIntersectionObserver(scrollHeight);
    //         return;
    //       }
    //       let index = findIndex(entries);
    //       activateNavByIndex(navItems[index]);
    //     }, {
    //       rootMargin: marginTop + "px 0px -100% 0px",
    //       threshold: 0,
    //     }
    //   );
    //   sections.forEach((element) => {
    //     element && intersectionObserver.observe(element);
    //   });
    // }
    // createIntersectionObserver(document.documentElement.scrollHeight);
  }

  document.addEventListener("DOMContentLoaded", ()=>{
    volantis.requestAnimationFrame(listennSidebarTOC)
  });
  document.addEventListener("pjax:success", ()=>{
    volantis.requestAnimationFrame(listennSidebarTOC)
  });
</script>



<script>
  document.onreadystatechange = function () {
    if (document.readyState == 'complete') {
      // 页面加载完毕 样式加载失败，或是当前网速慢，或是开启了省流模式
      const { saveData, effectiveType } = navigator.connection || navigator.mozConnection || navigator.webkitConnection || {}
      if (getComputedStyle(document.querySelector("#safearea"), null)["display"] == "none" || saveData || /2g/.test(effectiveType)) {
        document.querySelectorAll(".reveal").forEach(function (e) {
          e.style["opacity"] = "1";
        });
        document.querySelector("#safearea").style["display"] = "block";
      }
    }
  }
</script>


  <script type="application/ld+json">[{"@context":"http://schema.org","@type":"Organization","name":"MengFanjun的博客","url":"https://mengfanjun020906.github.io/","logo":{"@type":"ImageObject","url":"https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png","width":192,"height":192}},{"@context":"http://schema.org","@type":"Person","name":"MengFanjun","image":{"@type":"ImageObject","url":"https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png"},"url":"https://mengfanjun020906.github.io/","sameAs":["https://github.com/volantis-x"],"description":"20级通信工程大学生的个人博客"},{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://mengfanjun020906.github.io/","name":"MengFanjun的博客"}},{"@type":"ListItem","position":3,"item":{"@id":"https://mengfanjun020906.github.io/2025/08/30/Standford CS336（二）训练模型介绍/","name":"Standford CS336（二）训练模型介绍"}}]},{"@context":"http://schema.org","@type":"WebSite","name":"MengFanjun的博客","url":"https://mengfanjun020906.github.io/","keywords":null,"description":"20级通信工程大学生的个人博客","author":{"@type":"Person","name":"MengFanjun","image":{"@type":"ImageObject","url":"https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png"},"url":"https://mengfanjun020906.github.io/","description":"20级通信工程大学生的个人博客"},"publisher":{"@type":"Organization","name":"MengFanjun的博客","url":"https://mengfanjun020906.github.io/","logo":{"@type":"ImageObject","url":"https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png","width":192,"height":192}},"potentialAction":{"@type":"SearchAction","name":"Site Search","target":{"@type":"EntryPoint","urlTemplate":"https://mengfanjun020906.github.io?s={search_term_string}"},"query-input":"required name=search_term_string"}},{"@context":"http://schema.org","@type":"BlogPosting","headline":"Standford CS336（二）训练模型介绍","description":"20级通信工程大学生的个人博客","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https://mengfanjun020906.github.io/2025/08/30/Standford%20CS336%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/"},"author":{"@type":"Person","name":"MengFanjun","image":{"@type":"ImageObject","url":"https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png"},"url":"https://mengfanjun020906.github.io/"},"publisher":{"@type":"Organization","name":"MengFanjun的博客","logo":{"@type":"ImageObject","url":"https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png","width":192,"height":192}},"url":"https://mengfanjun020906.github.io/2025/08/30/Standford%20CS336%EF%BC%88%E4%BA%8C%EF%BC%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/","wordCount":0,"datePublished":"2025-08-30T02:57:21.000Z","dateModified":"2025-12-12T14:25:01.045Z","keywords":"学习,大模型","image":{"@type":"ImageObject","url":"https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/blog/favicon/android-chrome-192x192.png","width":192,"height":192}}]</script>



      
        <!--
  pjax重载区域接口：
  1.  <pjax></pjax> 标签 pjax 标签必须存在于所有页面 否则 pjax error
  2.  script[data-pjax]
  3.  .pjax-reload script
  4.  .pjax
-->



<script src="https://unpkg.com/volantis-static@0.0.1654736714924/libs/pjax/pjax.min.js"></script>


<script>
    var pjax;
    document.addEventListener('DOMContentLoaded', function () {
      pjax = new Pjax({
        elements: 'a[href]:not([href^="#"]):not([href="javascript:void(0)"]):not([pjax-fancybox]):not([onclick="return false;"]):not([onclick="return!1"]):not([target="_blank"]):not([target="view_window"]):not([href$=".xml"])',
        selectors: [
          "head title",
          "head meta[name=keywords]",
          "head meta[name=description]",
          
          "#l_main",
          "#pjax-header-nav-list",
          ".pjax",
          "pjax", // <pjax></pjax> 标签
          "script[data-pjax], .pjax-reload script" // script标签添加data-pjax 或 script标签外层添加.pjax-reload 的script代码段重载
        ],
        cacheBust: false,   // url 地址追加时间戳，用以避免浏览器缓存
        timeout: 5000,
        
      });
    });

    document.addEventListener('pjax:send', function (e) {
      //window.stop(); // 相当于点击了浏览器的停止按钮

      try {
        var currentUrl = window.location.pathname;
        var targetUrl = e.triggerElement.href;
        var banUrl = [""];
        if (banUrl[0] != "") {
          banUrl.forEach(item => {
            if(currentUrl.indexOf(item) != -1 || targetUrl.indexOf(item) != -1) {
              window.location.href = targetUrl;
            }
          });
        }
      } catch (error) {}

      // 使用 volantis.pjax.send 方法传入pjax:send回调函数 参见layout/_partial/scripts/global.ejs
      volantis.pjax.method.send.start();
    });

    document.addEventListener('pjax:complete', function () {
      // 使用 volantis.pjax.push 方法传入重载函数 参见layout/_partial/scripts/global.ejs
      volantis.pjax.method.complete.start();
    });

    document.addEventListener('pjax:error', function (e) {
      if(volantis.debug) {
        console.error(e);
        console.log('pjax error: \n' + JSON.stringify(e));
      }else{
        // 使用 volantis.pjax.error 方法传入pjax:error回调函数 参见layout/_partial/scripts/global.ejs
        volantis.pjax.method.error.start();
        window.location.href = e.triggerElement.href;
      }
    });
</script>

      
    </div>
    <!-- import body_end begin-->
    <!-- import body_end end-->
    <!-- Custom Files bodyEnd begin-->
    
    <!-- Custom Files bodyEnd end-->
  </body>
</html>
